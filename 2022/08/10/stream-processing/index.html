<!DOCTYPE html>
<html>

  <head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <!-- Bing WebMaster -->
  <meta name="msvalidate.01" content="AB2FFF876C37F59D9121882CC8395DE5" />

  <title>Stream Processing</title>
  <meta name="description" content="">

  <link rel="stylesheet" href="/css/main.css">
  <link rel="canonical" href="https://blog.codefarm.me/2022/08/10/stream-processing/">
  <link rel="alternate" type="application/rss+xml" title="CODE FARM" href="https://blog.codefarm.me/feed.xml">

  <!-- https://cdn.jsdelivr.net/gh/lurongkai/anti-baidu/js/anti-baidu-latest.min.js -->
<script type="text/javascript" src="/js/anti-baidu.min.js" charset="UTF-8"></script>

  
<!-- Google Analytics Website tracking -->
<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-83971182-1', 'auto');
  ga('send', 'pageview');

</script>


  
<!-- Google tag (gtag.js) -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-SN88FJ18E5"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-SN88FJ18E5');
</script>



</head>


  <body>

    <header class="site-header">

  <div class="wrapper">
    <h2 class="site-title">
      <a class="site-title" href="/">CODE FARM</a>
    </h2>

     <nav class="site-nav">
      <a href="#" class="menu-icon">
        <svg viewBox="0 0 18 15">
          <path fill="#424242" d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.031C17.335,0,18,0.665,18,1.484L18,1.484z"/>
          <path fill="#424242" d="M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0c0-0.82,0.665-1.484,1.484-1.484 h15.031C17.335,6.031,18,6.696,18,7.516L18,7.516z"/>
          <path fill="#424242" d="M18,13.516C18,14.335,17.335,15,16.516,15H1.484C0.665,15,0,14.335,0,13.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.031C17.335,12.031,18,12.696,18,13.516L18,13.516z"/>
        </svg>
      </a>
        <div class="trigger">
            <ul>
                <li><a href="/">home</a>
                <li><a href="/category">category</a>
                <li><a href="/tag">tag</a>
                <li><a href="/archive">archive</a>
                <li><a href="/about">about</a>
                <li><a href="https://resume.github.io/?qqbuby" target="_blank">R&eacute;sum&eacute;</a>
            </ul>
      </div>
    </nav>

  </div>

</header>


    <div class="page-content">
      <div class="wrapper">
        <article class="post" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title" itemprop="name headline">Stream Processing</h1>
    
    
    <p class="post-meta"><time datetime="2022-08-10T08:29:55+08:00" itemprop="datePublished">Aug 10, 2022</time></p>
  </header>

  <div class="post-content" itemprop="articleBody">
    <div id="toc" class="toc">
<div id="toctitle"></div>
<ul class="sectlevel1">
<li><a href="#transmitting-event-streams">1. Transmitting Event Streams</a>
<ul class="sectlevel2">
<li><a href="#messaging-systems">1.1. Messaging Systems</a>
<ul class="sectlevel3">
<li><a href="#direct-messaging-from-producers-to-consumers">1.1.1. Direct messaging from producers to consumers</a></li>
<li><a href="#message-brokers">1.1.2. Message brokers</a></li>
<li><a href="#message-brokers-compared-to-databases">1.1.3. Message brokers compared to databases</a></li>
<li><a href="#multiple-consumers">1.1.4. Multiple consumers</a></li>
<li><a href="#acknowledgments-and-redelivery">1.1.5. Acknowledgments and redelivery</a></li>
</ul>
</li>
<li><a href="#partitioned-logs">1.2. Partitioned Logs</a>
<ul class="sectlevel3">
<li><a href="#using-logs-for-message-storage">1.2.1. Using logs for message storage</a></li>
<li><a href="#logs-compared-to-traditional-messaging">1.2.2. Logs compared to traditional messaging</a></li>
<li><a href="#consumer-offsets">1.2.3. Consumer offsets</a></li>
<li><a href="#disk-space-usage">1.2.4. Disk space usage</a></li>
<li><a href="#when-consumers-cannot-keep-up-with-producers">1.2.5. When consumers cannot keep up with producers</a></li>
<li><a href="#replaying-old-messages">1.2.6. Replaying old messages</a></li>
</ul>
</li>
<li><a href="#amqpjms-style-message-broker-vs-log-based-message-broker">1.3. AMQP/JMS-style message broker vs Log-based message broker</a></li>
</ul>
</li>
<li><a href="#databases-and-streams">2. Databases and Streams</a>
<ul class="sectlevel2">
<li><a href="#keeping-systems-in-sync">2.1. Keeping Systems in Sync</a></li>
<li><a href="#change-data-capture">2.2. Change Data Capture</a>
<ul class="sectlevel3">
<li><a href="#implementing-change-data-capture">2.2.1. Implementing change data capture</a></li>
<li><a href="#log-compaction">2.2.2. Log compaction</a></li>
<li><a href="#api-support-for-change-streams">2.2.3. API support for change streams</a></li>
</ul>
</li>
<li><a href="#event-sourcing">2.3. Event Sourcing</a>
<ul class="sectlevel3">
<li><a href="#deriving-current-state-from-the-event-log">2.3.1. Deriving current state from the event log</a></li>
<li><a href="#commands-and-events">2.3.2. Commands and events</a></li>
</ul>
</li>
<li><a href="#state-streams-and-immutability">2.4. State, Streams, and Immutability</a>
<ul class="sectlevel3">
<li><a href="#advantages-of-immutable-events">2.4.1. Advantages of immutable events</a></li>
<li><a href="#deriving-several-views-from-the-same-event-log">2.4.2. Deriving several views from the same event log</a></li>
<li><a href="#concurrency-control">2.4.3. Concurrency control</a></li>
<li><a href="#limitations-of-immutability">2.4.4. Limitations of immutability</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#processing-streams">3. Processing Streams</a>
<ul class="sectlevel2">
<li><a href="#uses-of-stream-processing">3.1. Uses of Stream Processing</a>
<ul class="sectlevel3">
<li><a href="#complex-event-processing">3.1.1. Complex event processing</a></li>
<li><a href="#stream-analytics">3.1.2. Stream analytics</a></li>
<li><a href="#reasoning-about-time">3.1.3. Reasoning About Time</a></li>
<li><a href="#types-of-windows">3.1.4. Types of windows</a></li>
</ul>
</li>
<li><a href="#stream-joins">3.2. Stream Joins</a></li>
<li><a href="#fault-tolerance">3.3. Fault Tolerance</a>
<ul class="sectlevel3">
<li><a href="#microbatching-and-checkpointing">3.3.1. Microbatching and checkpointing</a></li>
<li><a href="#atomic-commit-revisited">3.3.2. Atomic commit revisited</a></li>
<li><a href="#idempotence">3.3.3. Idempotence</a></li>
<li><a href="#rebuilding-state-after-a-failure">3.3.4. Rebuilding state after a failure</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#what-is-apache-kafka">4. What Is Apache Kafka?</a>
<ul class="sectlevel2">
<li><a href="#the-producer">4.1. The Producer</a></li>
<li><a href="#the-consumer">4.2. The Consumer</a></li>
<li><a href="#message-delivery-semantics">4.3. Message Delivery Semantics</a></li>
</ul>
</li>
<li><a href="#references">5. References</a></li>
</ul>
</div>
<div id="preamble">
<div class="sectionbody">
<div class="paragraph">
<p>In reality, a lot of data is <strong>unbounded</strong> because it arrives gradually over time: your users produced data yesterday and today, and they will continue to produce more data tomorrow. Unless you go out of business, this process never ends, and so the dataset is never “complete” in any meaningful way. Thus, <strong>batch processors</strong> must artificially divide the data into chunks of fixed duration: for example, processing a day’s worth of data at the end of every day, or processing an hour’s worth of data at the end of every hour.</p>
</div>
<div class="paragraph">
<p>The problem with daily batch processes is that changes in the input are only reflected in the output a day later, which is too slow for many impatient users. To reduce the delay, we can run the processing more frequently—say, processing a second’s worth of data at the end of every second—or even continuously, abandoning the fixed time slices entirely and simply processing every event as it happens. That is the idea behind <strong>stream processing</strong>.</p>
</div>
</div>
</div>
<div class="sect1">
<h2 id="transmitting-event-streams">1. Transmitting Event Streams</h2>
<div class="sectionbody">
<div class="paragraph">
<p>In the batch processing world, the inputs and outputs of a job are files (perhaps on a distributed filesystem). What does the streaming equivalent look like?</p>
</div>
<div class="paragraph">
<p>When the input is a file (a sequence of bytes), the first processing step is usually to parse it into a sequence of records. In a stream processing context, a record is more commonly known as an <strong>event</strong>, but it is essentially the same thing: a small, selfcontained, immutable object containing the details of something that happened at some point in time. An event usually contains a timestamp indicating when it happened according to a time-of-day clock.</p>
</div>
<div class="paragraph">
<p>In batch processing, a file is written once and then potentially read by multiple jobs. Analogously, in streaming terminology, an event is generated once by a <strong>producer</strong> (also known as a <strong>publisher</strong> or <strong>sender</strong>), and then potentially processed by multiple <strong>consumers</strong> (<strong>subscribers</strong> or <strong>recipients</strong>). In a filesystem, a filename identifies a set of related records; in a streaming system, related events are usually grouped together into a <strong>topic</strong> or <strong>stream</strong>.</p>
</div>
<div class="sect2">
<h3 id="messaging-systems">1.1. Messaging Systems</h3>
<div class="paragraph">
<p>A common approach for notifying consumers about new events is to use a <strong>messaging system</strong>: a producer sends a message containing the event, which is then pushed to consumers.</p>
</div>
<div class="paragraph">
<p>Within this <strong>publish/subscribe</strong> model, different systems take a wide range of approaches, and there is no one right answer for all purposes. To differentiate the
systems, it is particularly helpful to ask the following two questions:</p>
</div>
<div class="olist arabic">
<ol class="arabic">
<li>
<p>What happens if the <strong>producers send messages faster than the consumers</strong> can process them?</p>
<div class="paragraph">
<p>Broadly speaking, there are three options: the system can <strong>drop</strong> messages, <strong>buffer</strong> messages in a queue, or apply <strong>backpressure</strong>.</p>
</div>
<div class="paragraph">
<p>If messages are buffered in a queue, it is important to understand what happens as that queue grows. Does the system crash if the queue no longer fits in memory, or does it write messages to disk? If so, how does the disk access affect the performance of the messaging system?</p>
</div>
</li>
<li>
<p>What happens if <strong>nodes crash</strong> or temporarily go offline—are any messages lost?</p>
<div class="paragraph">
<p>As with databases, <strong>durability</strong> may require some combination of writing to disk and/or replication, which has a cost. If you can afford to sometimes lose messages, you can probably get higher throughput and lower latency on the same hardware.</p>
</div>
</li>
</ol>
</div>
<div class="sect3">
<h4 id="direct-messaging-from-producers-to-consumers">1.1.1. Direct messaging from producers to consumers</h4>
<div class="paragraph">
<p>A number of messaging systems use direct network communication between producers and consumers without going via intermediary nodes:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>UDP multicast is widely used in the financial industry for streams such as stock market feeds, where low latency is important. Although UDP itself is unreliable, application-level protocols can recover lost packets (the producer must remember packets it has sent so that it can retransmit them on demand).</p>
</li>
<li>
<p>Brokerless messaging libraries such as <strong>ZeroMQ</strong> and nanomsg take a similar approach, implementing publish/subscribe messaging over TCP or IP multicast.</p>
</li>
</ul>
</div>
</div>
<div class="sect3">
<h4 id="message-brokers">1.1.2. Message brokers</h4>
<div class="paragraph">
<p>A widely used alternative is to send messages via a <strong>message broker</strong> (also known as a <strong>message queue</strong>), which is essentially a kind of database that is optimized for handling message streams. It runs as a server, with producers and consumers connecting to it as clients. Producers write messages to the broker, and consumers receive them by reading them from the broker.</p>
</div>
<div class="paragraph">
<p>By centralizing the data in the broker, these systems can more easily <strong>tolerate clients</strong> that come and go (connect, disconnect, and crash), and the question of <strong>durability</strong> is moved to the broker instead. Some message brokers only keep messages in memory, while others (depending on configuration) write them to disk so that they are not lost in case of a broker crash. Faced with slow consumers, they generally allow <strong>unbounded queueing</strong> (as opposed to dropping messages or backpressure), although this choice may also depend on the configuration.</p>
</div>
<div class="paragraph">
<p>A consequence of queueing is also that consumers are generally <strong>asynchronous</strong>: when a producer sends a message, it normally only waits for the broker to confirm that it has buffered the message and does not wait for the message to be processed by consumers. The delivery to consumers will happen at some undetermined future point in time—often within a fraction of a second, but sometimes significantly later if there is a queue backlog.</p>
</div>
</div>
<div class="sect3">
<h4 id="message-brokers-compared-to-databases">1.1.3. Message brokers compared to databases</h4>
<div class="paragraph">
<p>Some message brokers can even participate in <strong>two-phase commit</strong> protocols using XA or JTA. This feature makes them quite similar in nature to databases, although there are still important practical differences between message brokers and databases:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>Databases usually keep data until it is explicitly deleted, whereas most message brokers automatically delete a message when it has been successfully delivered to
its consumers. Such message brokers are not suitable for <strong>long-term data storage</strong>.</p>
</li>
<li>
<p>Since they quickly delete messages, most message brokers assume that their working set is fairly small—i.e., the queues are short. If the broker needs to buffer a lot of messages because the consumers are slow (perhaps spilling messages to disk if they no longer fit in memory), each individual message takes longer to process, and the overall throughput may degrade.</p>
</li>
<li>
<p>Databases often support secondary indexes and various ways of searching for data, while message brokers often support some way of subscribing to a subset of topics matching some pattern. The mechanisms are different, but both are essentially ways for a client to select the portion of the data that it wants to know about.</p>
</li>
<li>
<p>When querying a database, the result is typically based on a point-in-time snapshot of the data; if another client subsequently writes something to the database that changes the query result, the first client does not find out that its prior result is now outdated (unless it repeats the query, or polls for changes). By contrast, message brokers do not support arbitrary queries, but they do notify clients when data changes (i.e., when new messages become available).</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>This is the traditional view of message brokers, which is encapsulated in standards like <strong>JMS</strong> and <strong>AMQP</strong> and implemented in software like <strong>RabbitMQ</strong>, <strong>ActiveMQ</strong>, HornetQ, Qpid, TIBCO Enterprise Message Service, <strong>IBM MQ</strong>, Azure Service Bus, and Google Cloud Pub/Sub.</p>
</div>
</div>
<div class="sect3">
<h4 id="multiple-consumers">1.1.4. Multiple consumers</h4>
<div class="paragraph">
<p>When multiple consumers read messages in the same topic, two main patterns of messaging are used, as illustrated in Figure 11-1:</p>
</div>
<div class="imageblock">
<div class="content">
<img src="/assets/ddia/stream-processing/Figure_11-1_message_load_balancing_fan_out.png" alt="Figure 11 1 message load balancing fan out" width="75%" height="75%">
</div>
</div>
<div class="ulist">
<ul>
<li>
<p><strong>Load balancing</strong></p>
<div class="paragraph">
<p>Each message is delivered to one of the consumers, so the consumers can share the work of processing the messages in the topic. The broker may assign messages to consumers arbitrarily. This pattern is useful when the messages are expensive to process, and so you want to be able to add consumers to parallelize the processing.</p>
</div>
</li>
<li>
<p><strong>Fan-out</strong></p>
<div class="paragraph">
<p>Each message is delivered to all of the consumers. Fan-out allows several independent consumers to each “tune in” to the same broadcast of messages, without affecting each other—the streaming equivalent of having several different batch jobs that read the same input file.</p>
</div>
</li>
</ul>
</div>
<div class="paragraph">
<p>The two patterns can be combined: for example, <strong>two separate <mark>groups of consumers</mark> may each subscribe to a topic</strong>, such that each group collectively receives all messages, but <strong>within each group only one of the nodes receives each message</strong>.</p>
</div>
</div>
<div class="sect3">
<h4 id="acknowledgments-and-redelivery">1.1.5. Acknowledgments and redelivery</h4>
<div class="paragraph">
<p>Consumers may crash at any time, so it could happen that a broker delivers a message to a consumer but the consumer never processes it, or only partially processes it before crashing. In order to ensure that the message is not lost, message brokers use <strong>acknowledgments</strong>: a client must explicitly tell the broker when it has finished processing a message so that the broker can remove it from the queue.</p>
</div>
<div class="paragraph">
<p>If the connection to a client is closed or times out without the broker receiving an acknowledgment, it assumes that the message was not processed, and therefore it delivers the message again to another consumer. Note that it could happen that the message actually was fully processed, but the acknowledgment was lost in the network. Handling this case requires an <strong>atomic commit</strong> protocol.</p>
</div>
<div class="paragraph">
<p>When combined with load balancing, this redelivery behavior has an interesting effect on <strong>the ordering of messages</strong>.</p>
</div>
<div class="imageblock">
<div class="content">
<img src="/assets/ddia/stream-processing/Figure_11-2_consumer_crashes_order_of_message.png" alt="Figure 11 2 consumer crashes order of message" width="75%" height="75%">
</div>
</div>
<div class="paragraph">
<p>Even if the message broker otherwise tries to preserve the order of messages, the combination of load balancing with redelivery inevitably leads to messages being reordered. To avoid this issue, you can <strong>use a separate queue per consumer</strong> (i.e., not use the load balancing feature). Message reordering is not a problem if messages are completely independent of each other, but it can be important if there are <strong>causal dependencies between messages</strong>.</p>
</div>
</div>
</div>
<div class="sect2">
<h3 id="partitioned-logs">1.2. Partitioned Logs</h3>
<div class="paragraph">
<p>Sending a packet over a network or making a request to a network service is normally a transient operation that leaves no permanent trace. Although it is possible to record it permanently (using packet capture and logging), we normally don’t think of it that way. Even message brokers that durably write messages to disk quickly delete them again after they have been delivered to consumers, because they are built around a <strong>transient messaging</strong> mindset.</p>
</div>
<div class="paragraph">
<p>Databases and filesystems take the opposite approach: everything that is written to a database or file is normally expected to be <strong>permanently recorded</strong>, at least until someone explicitly chooses to delete it again.</p>
</div>
<div class="paragraph">
<p>Why can we not have a hybrid, combining the durable storage approach of databases with the low-latency notification facilities of messaging? This is the idea behind <strong>logbased message brokers</strong>.</p>
</div>
<div class="sect3">
<h4 id="using-logs-for-message-storage">1.2.1. Using logs for message storage</h4>
<div class="paragraph">
<p>A log is simply an append-only sequence of records on disk, e.g. log-structured storage engines and write-ahead logs.</p>
</div>
<div class="paragraph">
<p>The same structure can be used to implement a message broker: a producer sends a message by appending it to the end of the log, and a consumer receives messages by reading the log sequentially. If a consumer reaches the end of the log, it waits for a notification that a new message has been appended. The Unix tool <strong>tail -f</strong>, which watches a file for data being appended, essentially works like this.</p>
</div>
<div class="paragraph">
<p>In order to scale to higher throughput than a single disk can offer, the log can be <strong>partitioned</strong>. Different partitions can then be hosted on different machines, making each partition a separate log that can be read and written independently from other partitions. A topic can then be defined as a group of partitions that all carry messages of the same type.</p>
</div>
<div class="paragraph">
<p>Within each partition, the broker assigns a monotonically increasing sequence number, or <strong>offset</strong>, to every message. Such a sequence number makes sense because a partition is append-only, so <strong>the messages within a partition are totally ordered</strong>. There is <strong>no ordering guarantee across different partitions</strong>.</p>
</div>
<div class="imageblock">
<div class="content">
<img src="/assets/ddia/stream-processing/Figure_11-3_topic_partion_sequential.png" alt="Figure 11 3 topic partion sequential" width="75%" height="75%">
</div>
</div>
<div class="paragraph">
<p>Apache <strong>Kafka</strong>, Amazon Kinesis Streams, and Twitter’s DistributedLog are log-based message brokers that work like this. Google Cloud Pub/Sub is architecturally similar but exposes a JMS-style API rather than a log abstraction. Even though these message brokers write all messages to disk, they are able to <strong>achieve throughput of millions of messages per second by partitioning across multiple machines</strong>, and <strong>fault tolerance by replicating messages</strong>.</p>
</div>
</div>
<div class="sect3">
<h4 id="logs-compared-to-traditional-messaging">1.2.2. Logs compared to traditional messaging</h4>
<div class="paragraph">
<p>The log-based approach trivially supports <strong>fan-out</strong> messaging, because several consumers can independently read the log without affecting each other—reading a message does not delete it from the log. To achieve <strong>load balancing</strong> across a group of consumers, instead of assigning individual messages to consumer clients, the broker can assign entire partitions to nodes in the <strong>consumer group</strong>.</p>
</div>
<div class="paragraph">
<p>Each client then consumes all the messages in the partitions it has been assigned. Typically, when a consumer has been assigned a log partition, it <strong>reads the messages in the partition sequentially</strong>, in a straightforward <strong>single-threaded</strong> manner. This coarse grained load balancing approach has some downsides:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>The number of nodes sharing the work of consuming a topic can be at most the number of log partitions in that topic, because messages within the same partition
are delivered to the same node.</p>
</li>
<li>
<p>If a single message is slow to process, it holds up the processing of subsequent messages in that partition (HOL, a form of <strong>head-of-line blocking</strong>).</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>Thus, in situations where messages may be expensive to process and you want to parallelize processing on a message-by-message basis, and where message ordering is not so important, the JMS/AMQP style of message broker is preferable. On the other hand, in situations with high message throughput, where each message is fast to process and where message ordering is important, the log-based approach works very well.</p>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
<div class="paragraph">
<p>It’s possible to create a load balancing scheme in which two consumers share the work of processing a partition by having both read the full set of messages, but one of them only considers messages with even-numbered offsets while the other deals with the odd-numbered offsets. Alternatively, you could spread message processing over a thread pool, but that approach complicates consumer offset management.</p>
</div>
<div class="paragraph">
<p>In general, <strong>single-threaded processing of a partition is preferable, and parallelism can be increased by using more partitions.</strong></p>
</div>
</td>
</tr>
</table>
</div>
</div>
<div class="sect3">
<h4 id="consumer-offsets">1.2.3. Consumer offsets</h4>
<div class="paragraph">
<p>Consuming a partition sequentially makes it easy to tell which messages have been processed: all messages with an offset less than a consumer’s current offset have
already been processed, and all messages with a greater offset have not yet been seen.</p>
</div>
<div class="paragraph">
<p>Thus, the broker does not need to track acknowledgments for every single message—it only needs to periodically record the <strong>consumer offsets</strong>. The reduced bookkeeping overhead and the opportunities for batching and pipelining in this approach help increase the throughput of log-based systems.</p>
</div>
<div class="paragraph">
<p>This offset is in fact very similar to the <strong>log sequence number</strong> that is commonly found in single-leader database replication. In database replication, the log sequence number allows a follower to reconnect to a leader after it has become disconnected, and resume replication without skipping any writes. Exactly the same principle is used here: the message broker behaves like a leader database, and the consumer like a follower.</p>
</div>
<div class="paragraph">
<p>If a consumer node fails, another node in the consumer group is assigned the failed consumer’s partitions, and it starts consuming messages at the last recorded offset. If the consumer had processed subsequent messages but not yet recorded their offset, those messages will be processed a second time upon restart.</p>
</div>
</div>
<div class="sect3">
<h4 id="disk-space-usage">1.2.4. Disk space usage</h4>
<div class="paragraph">
<p>If you only ever append to the log, you will eventually run out of disk space. To reclaim disk space, the log is actually divided into <strong>segments</strong>, and from time to time old segments are deleted or moved to archive storage.</p>
</div>
<div class="paragraph">
<p>This means that if a slow consumer cannot keep up with the rate of messages, and it falls so far behind that its consumer offset points to a deleted segment, it will miss some of the messages. Effectively, the log implements a bounded-size buffer that discards old messages when it gets full, also known as a <strong>circular buffer</strong> or <strong>ring buffer</strong>. However, since that buffer is on disk, it can be quite large.</p>
</div>
<div class="paragraph">
<p>Let’s do a back-of-the-envelope calculation. At the time of writing, a typical large hard drive has a capacity of 6 TB and a sequential write throughput of 150 MB/s. If you are writing messages at the fastest possible rate, it takes about 11 hours to fill the drive. Thus, the disk can buffer 11 hours’ worth of messages, after which it will start overwriting old messages. This ratio remains the same, even if you use many hard drives and machines. In practice, deployments rarely use the full write bandwidth of the disk, so the log can typically keep a buffer of several days’ or even weeks’worth of messages.</p>
</div>
<div class="paragraph">
<p>Regardless of how long you retain messages, the throughput of a log remains more or less constant, since every message is written to disk anyway. This behavior is in contrast to messaging systems that keep messages in memory by default and only write them to disk if the queue grows too large: such systems are fast when queues are short and become much slower when they start writing to disk, so the throughput depends on the amount of history retained.</p>
</div>
</div>
<div class="sect3">
<h4 id="when-consumers-cannot-keep-up-with-producers">1.2.5. When consumers cannot keep up with producers</h4>
<div class="paragraph">
<p>If a consumer falls so far behind that the messages it requires are older than what is retained on disk, it will not be able to read those messages—so the broker effectively drops old messages that go back further than the size of the buffer can accommodate. You can <strong>monitor</strong> how far a consumer is behind the head of the log, and raise an <strong>alert</strong> if it falls behind significantly. As the buffer is large, there is enough time for a <strong>human operator</strong> to fix the <strong>slow consumer</strong> and allow it to catch up before it starts missing messages.</p>
</div>
</div>
<div class="sect3">
<h4 id="replaying-old-messages">1.2.6. Replaying old messages</h4>
<div class="paragraph">
<p>With AMQP- and JMS-style message brokers, processing and acknowledging messages is a destructive operation, since it causes the messages to be deleted on the broker. On the other hand, in a log-based message broker, consuming messages is more like reading from a file: it is a read-only operation that does not change the log.</p>
</div>
<div class="paragraph">
<p>This aspect makes log-based messaging more like the batch processes, where derived data is clearly separated from input data through a repeatable transformation process. It allows more experimentation and easier recovery from errors and bugs, making it a good tool for integrating dataflows within an organization.</p>
</div>
</div>
</div>
<div class="sect2">
<h3 id="amqpjms-style-message-broker-vs-log-based-message-broker">1.3. AMQP/JMS-style message broker vs Log-based message broker</h3>
<div class="ulist">
<ul>
<li>
<p>AMQP/JMS-style message broker</p>
<div class="paragraph">
<p>The broker assigns individual messages to consumers, and consumers acknowledge individual messages when they have been successfully processed. Messages are deleted from the broker once they have been acknowledged. This approach is appropriate as an asynchronous form of RPC, for example in a task queue, where the exact order of message processing is not important and where there is no need to go back and read old messages again after they have been processed.</p>
</div>
<div class="ulist">
<ul>
<li>
<p><a href="https://stackoverflow.com/questions/15150133/jms-and-amqp-rabbitmq" class="bare">https://stackoverflow.com/questions/15150133/jms-and-amqp-rabbitmq</a></p>
</li>
</ul>
</div>
</li>
<li>
<p>Log-based message broker</p>
<div class="paragraph">
<p>The broker assigns all messages in a partition to the same consumer node, and always delivers messages in the same order. Parallelism is achieved through partitioning, and consumers track their progress by checkpointing the offset of the last message they have processed. The broker retains messages on disk, so it is possible to jump back and reread old messages if necessary.</p>
</div>
<div class="ulist">
<ul>
<li>
<p><a href="https://www.simplilearn.com/kafka-vs-rabbitmq-article" class="bare">https://www.simplilearn.com/kafka-vs-rabbitmq-article</a></p>
</li>
</ul>
</div>
</li>
</ul>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="databases-and-streams">2. Databases and Streams</h2>
<div class="sectionbody">
<div class="paragraph">
<p>We have drawn some comparisons between message brokers and databases. Even though they have traditionally been considered separate categories of tools, we saw that <strong>log-based message brokers</strong> have been successful in taking ideas from databases and applying them to messaging. We can also go in reverse: take ideas from messaging and streams, and apply them to databases.</p>
</div>
<div class="paragraph">
<p>In fact, a <strong>replication log</strong> is a stream of database write events, produced by the leader as it processes transactions. The followers apply that stream of writes to their own copy of the database and thus end up with an accurate copy of the same data. The events in the replication log describe the data changes that occurred.</p>
</div>
<div class="paragraph">
<p>The <strong>state machine replication</strong> principle with <strong>total order</strong> broadcast, which states: if every event represents a write to the database, and every replica processes the same events in the same order, then the replicas will all end up in the same final state. It’s just another case of event streams.</p>
</div>
<div class="sect2">
<h3 id="keeping-systems-in-sync">2.1. Keeping Systems in Sync</h3>
<div class="paragraph">
<p>There is no single system that can satisfy all data storage, querying, and processing needs, and most nontrivial applications need to combine several different technologies in order to satisfy their requirements: for example, using an <strong>OLTP database to serve user requests</strong>, a <strong>cache to speed up common requests</strong>, a <strong>full-text index to handle search queries</strong>, and a <strong>data warehouse for analytics</strong>. Each of these has its own copy of the data, stored in its own representation that is optimized for its own purposes.</p>
</div>
<div class="paragraph">
<p>As the same or related data appears in several different places, they need to be <strong>kept in sync</strong> with one another: if an item is updated in the database, it also needs to be updated in the cache, search indexes, and data warehouse. With data warehouses this synchronization is usually performed by ETL processes, often by taking a full copy of a database, transforming it, and bulk-loading it into the data warehouse—in other words, a batch process. Similarly, search indexes, recommendation systems, and other derived data systems might be created using batch processes.</p>
</div>
<div class="paragraph">
<p>If periodic full database dumps are too slow, an alternative that is sometimes used is <strong>dual writes</strong>, in which the application code explicitly writes to each of the systems when data changes: for example, first writing to the database, then updating the search index, then invalidating the cache entries (or even performing those writes concurrently).</p>
</div>
<div class="paragraph">
<p>However, dual writes have some serious problems, one of which is a <strong>race condition</strong>.</p>
</div>
<div class="imageblock">
<div class="content">
<img src="/assets/ddia/stream-processing/Figure_11-4_dual_write_race_condition.png" alt="Figure 11 4 dual write race condition" width="75%" height="75%">
</div>
</div>
<div class="paragraph">
<p>Another problem with dual writes is that one of the writes may fail while the other succeeds. This is a <strong>fault-tolerance</strong> problem rather than a concurrency problem, but it also has the effect of the two systems becoming inconsistent with each other. Ensuring that they either both succeed or both fail is a case of the <strong>atomic commit</strong> problem, which is expensive to solve.</p>
</div>
</div>
<div class="sect2">
<h3 id="change-data-capture">2.2. Change Data Capture</h3>
<div class="paragraph">
<p>The problem with most databases’ <strong>replication logs</strong> is that they have long been considered to be an internal implementation detail of the database, not a public API. Clients are supposed to query the database through its data model and query language, not parse the replication logs and try to extract data from them.</p>
</div>
<div class="paragraph">
<p>For decades, many databases simply did not have a documented way of getting the log of changes written to them. For this reason it was difficult to take all the changes made in a database and replicate them to a different storage technology such as a search index, cache, or data warehouse.</p>
</div>
<div class="paragraph">
<p>More recently, there has been growing interest in <strong>change data capture</strong> (CDC), which is the process of observing all data changes written to a database and extracting them in a form in which they can be replicated to other systems. CDC is especially interesting if changes are made available as a <strong>stream</strong>, immediately as they are written.</p>
</div>
<div class="imageblock">
<div class="content">
<img src="/assets/ddia/stream-processing/Figure_11-5_change-data-capture.png" alt="Figure 11 5 change data capture" width="75%" height="75%">
</div>
</div>
<div class="sect3">
<h4 id="implementing-change-data-capture">2.2.1. Implementing change data capture</h4>
<div class="paragraph">
<p>We can call the log consumers <strong>derived data systems</strong>: the data stored in the search index and the data warehouse is just another view onto the data in the system of record. Change data capture is a mechanism for ensuring that all changes made to the system of record are also reflected in the derived data systems so that the derived systems have an accurate copy of the data.</p>
</div>
<div class="paragraph">
<p>Essentially, change data capture makes one database the <strong>leader</strong> (the one from which the changes are captured), and turns the others into <strong>followers</strong>. A <strong>log-based message broker</strong> is well suited for transporting the change events from the source database, since it preserves the <strong>ordering of messages</strong>.</p>
</div>
<div class="paragraph">
<p>Database triggers can be used to implement change data capture by registering triggers that observe all changes to data tables and add corresponding entries to a changelog table. However, they tend to be fragile and have significant performance overheads. Parsing the <strong>replication log</strong> can be a more robust approach, although it also comes with challenges, such as handling schema changes.</p>
</div>
<div class="paragraph">
<p>LinkedIn’s Databus, Facebook’s Wormhole, and Yahoo!’s Sherpa use this idea at large scale. Bottled Water implements CDC for PostgreSQL using an API that decodes the <strong>write-ahead log</strong>, Maxwell and Debezium do something similar for MySQL by parsing the <strong>binlog</strong>, Mongoriver reads the MongoDB <strong>oplog</strong>, and GoldenGate provides similar facilities for Oracle.</p>
</div>
</div>
<div class="sect3">
<h4 id="log-compaction">2.2.2. Log compaction</h4>
<div class="paragraph">
<p>The principle of <strong>log compaction</strong> in the context of <strong>log-structured storage engines</strong> is simple: the storage engine periodically looks for log records with the same key, throws away any duplicates, and keeps only the most recent update for each key. This compaction and merging process runs in the background.</p>
</div>
<div class="paragraph">
<p>In a log-structured storage engine, an update with a special null value (a <strong>tombstone</strong>) indicates that a key was deleted, and causes it to be removed during log compaction. But as long as a key is not overwritten or deleted, it stays in the log forever. The disk space required for such a compacted log depends only on the current contents of the database, not the number of writes that have ever occurred in the database. If the same key is frequently overwritten, previous values will eventually be garbage collected, and only the latest value will be retained.</p>
</div>
<div class="paragraph">
<p>The same idea works in the context of <strong>log-based message brokers</strong> and change data capture. If the CDC system is set up such that every change has a primary key, and every update for a key replaces the previous value for that key, then it’s sufficient to keep just the most recent write for a particular key.</p>
</div>
<div class="paragraph">
<p>This feature is supported by Apache <strong>Kafka</strong>. It allows the message broker to be used for durable storage, not just for transient messaging.</p>
</div>
</div>
<div class="sect3">
<h4 id="api-support-for-change-streams">2.2.3. API support for change streams</h4>
<div class="paragraph">
<p>Increasingly, databases are beginning to support change streams as a first-class interface, rather than the typical retrofitted and reverse-engineered CDC efforts. For example, RethinkDB allows queries to subscribe to notifications when the results of a query change, Firebase and CouchDB provide data synchronization based on a change feed that is also made available to applications, and Meteor uses the MongoDB oplog to subscribe to data changes and update the user interface.</p>
</div>
<div class="paragraph">
<p><strong>Kafka Connect</strong> is an effort to integrate change data capture tools for a wide range of database systems with Kafka. Once the stream of change events is in Kafka, it can be used to update derived data systems such as search indexes, and also feed into stream processing systems.</p>
</div>
</div>
</div>
<div class="sect2">
<h3 id="event-sourcing">2.3. Event Sourcing</h3>
<div class="paragraph">
<p>Similarly to change data capture, <strong>event sourcing</strong> involves storing all changes to the application state as a log of change events. The biggest difference is that event sourcing applies the idea at a different level of abstraction:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>In change data capture, the application uses the database in a <strong>mutable</strong> way, updating and deleting records at will. The log of changes is extracted from the database at a <strong>low level</strong> (e.g., by parsing the replication log), which ensures that the order of writes extracted from the database matches the order in which they were actually written, avoiding the race condition. The application writing to the database does not need to be aware that CDC is occurring.</p>
</li>
<li>
<p>In event sourcing, the application logic is explicitly built on the basis of <strong>immutable</strong> events that are written to an event log. In this case, the event store is <strong>appendonly</strong>, and updates or deletes are discouraged or prohibited. Events are designed to reflect things that happened at the <strong>application level</strong>, rather than low-level state changes.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>Event sourcing is a powerful technique for data modeling: from an application point of view it is more meaningful to record the user’s actions as immutable events, rather than recording the effect of those actions on a mutable database. Event sourcing makes it easier to <strong>evolve applications</strong> over time, helps with debugging by making it easier to understand after the fact why something happened, and guards against application bugs.</p>
</div>
<div class="sect3">
<h4 id="deriving-current-state-from-the-event-log">2.3.1. Deriving current state from the event log</h4>
<div class="paragraph">
<p>An event log by itself is not very useful, because users generally expect to see the current state of a system, not the history of modifications. For example, on a shopping website, users expect to be able to see the current contents of their cart, not an append-only list of all the changes they have ever made to their cart.</p>
</div>
<div class="paragraph">
<p>Thus, applications that use event sourcing need to take the log of events (representing the data <strong>written</strong> to the system) and transform it into application state that is suitable for showing to a user (the way in which data is <strong>read</strong> from the system). This transformation can use arbitrary logic, but it should be <strong>deterministic</strong> so that you can run it again and derive the same application state from the event log.</p>
</div>
<div class="paragraph">
<p>Applications that use event sourcing typically have some mechanism for storing <strong>snapshots</strong> of the current state that is derived from the log of events, so they don’t need to repeatedly reprocess the full log. However, this is only a performance optimization to speed up reads and recovery from crashes; the intention is that the system is able to store all raw events forever and reprocess the full event log whenever required.</p>
</div>
</div>
<div class="sect3">
<h4 id="commands-and-events">2.3.2. Commands and events</h4>
<div class="paragraph">
<p>The event sourcing philosophy is careful to distinguish between <strong>events</strong> and <strong>commands</strong>. When a request from a user first arrives, it is initially a command: at this
point it may still fail, for example because some integrity condition is violated. The application must first validate that it can execute the command. If the validation is successful and the command is accepted, it becomes an event, which is durable and immutable.</p>
</div>
<div class="paragraph">
<p>For example, if a user tries to register a particular username, or reserve a seat on an airplane or in a theater, then the application needs to check that the username or seat is not already taken. When that check has succeeded, the application can generate an event to indicate that a particular username was registered by a particular user ID, or that a particular seat has been reserved for a particular customer.</p>
</div>
<div class="paragraph">
<p>At the point when the event is generated, it becomes a <strong>fact</strong>. Even if the customer later decides to change or cancel the reservation, the fact remains true that they formerly held a reservation for a particular seat, and the change or cancellation is a separate event that is added later.</p>
</div>
</div>
</div>
<div class="sect2">
<h3 id="state-streams-and-immutability">2.4. State, Streams, and Immutability</h3>
<div class="paragraph">
<p>We normally think of databases as storing the current state of the application—this representation is optimized for reads, and it is usually the most convenient for serving queries. The nature of state is that it changes, so databases support updating and deleting data as well as inserting it. How does this fit with immutability?</p>
</div>
<div class="paragraph">
<p>No matter how the <strong>state</strong> changes, there was always a sequence of <strong>events</strong> that caused those changes. Even as things are done and undone, the <strong>fact</strong> remains true that those events occurred. The key idea is that <strong>mutable state</strong> and an append-only log of <strong>immutable events</strong> do not contradict each other: they are two sides of the same coin. The log of all changes, the <strong>changelog</strong>, represents the evolution of state over time.</p>
</div>
<div class="imageblock">
<div class="content">
<img src="/assets/ddia/stream-processing/Figure_11-6_relationship-current-state-event-stream.png" alt="Figure 11 6 relationship current state event stream" width="75%" height="75%">
</div>
</div>
<div class="paragraph">
<p>If you store the changelog durably, that simply has the effect of making the state <strong>reproducible</strong>. If you consider the log of events to be your system of record, and any mutable state as being derived from it, it becomes easier to reason about the flow of data through a system. As Pat Helland puts it:</p>
</div>
<div class="quoteblock">
<blockquote>
<div class="paragraph">
<p>Transaction logs record all the changes made to the database. High-speed appends are the only way to change the log. From this perspective, the contents of the database hold a caching of the latest record values in the logs. <mark>The truth is the log.</mark> The database is a cache of a subset of the log. That cached subset happens to be the latest value of each record and index value from the log.</p>
</div>
</blockquote>
</div>
<div class="sect3">
<h4 id="advantages-of-immutable-events">2.4.1. Advantages of immutable events</h4>
<div class="paragraph">
<p>Immutability in databases is an old idea. For example, accountants have been using immutability for centuries in financial bookkeeping. When a transaction occurs, it is recorded in an append-only <strong>ledger</strong>, which is essentially a log of events describing money, goods, or services that have changed hands. The accounts, such as profit and loss or the balance sheet, are derived from the transactions in the ledger by adding them up.</p>
</div>
<div class="paragraph">
<p>If a mistake is made, accountants don’t erase or change the incorrect transaction in the ledger—instead, they add another transaction that <strong>compensates</strong> for the mistake, for example refunding an incorrect charge. The incorrect transaction still remains in the ledger forever, because it might be important for <strong>auditing</strong> reasons. If incorrect figures, derived from the incorrect ledger, have already been published, then the figures for the next accounting period include a correction. This process is entirely normal in accounting.</p>
</div>
<div class="paragraph">
<p>Although such <strong>auditability</strong> is particularly important in financial systems, it is also beneficial for many other systems that are not subject to such strict regulation. If you accidentally deploy buggy code that writes bad data to a database, recovery is much harder if the code is able to destructively overwrite data.  With an append-only log of immutable events, it is much easier to diagnose what happened and recover from the problem.</p>
</div>
<div class="paragraph">
<p>Immutable events also capture more information than just the current state. For example, on a shopping website, a customer may add an item to their cart and then remove it again. Although the second event cancels out the first event from the point of view of order fulfillment, it may be useful to know for <strong>analytics</strong> purposes that the customer was considering a particular item but then decided against it. Perhaps they will choose to buy it in the future, or perhaps they found a substitute. This information is recorded in an <strong>event log</strong>, but would be lost in a database that deletes items when they are removed from the cart.</p>
</div>
</div>
<div class="sect3">
<h4 id="deriving-several-views-from-the-same-event-log">2.4.2. Deriving several views from the same event log</h4>
<div class="paragraph">
<p>Moreover, by separating mutable state from the immutable event log, you can derive several different read-oriented representations from the same log of events.</p>
</div>
<div class="paragraph">
<p>Storing data is normally quite straightforward if you don’t have to worry about how it is going to be queried and accessed; many of the complexities of schema design, indexing, and storage engines are the result of wanting to support certain query and access patterns. For this reason, you gain a lot of flexibility by separating the form in which data is written from the form it is read, and by allowing several different read views. This idea is sometimes known as <strong>command query responsibility segregation</strong> (CQRS).</p>
</div>
</div>
<div class="sect3">
<h4 id="concurrency-control">2.4.3. Concurrency control</h4>
<div class="paragraph">
<p>The biggest downside of event sourcing and change data capture is that the consumers of the event log are usually <strong>asynchronous</strong>, so there is a possibility that a user may make a write to the log, then read from a log-derived view and find that their write has not yet been reflected in the read view.</p>
</div>
<div class="paragraph">
<p>One solution would be to <strong>perform the updates of the read view synchronously</strong> with appending the event to the log. This requires a <strong>transaction</strong> to combine the writes into an <strong>atomic</strong> unit, so either you need to keep the event log and the read view in the same storage system, or you need a distributed transaction across the different systems.</p>
</div>
</div>
<div class="sect3">
<h4 id="limitations-of-immutability">2.4.4. Limitations of immutability</h4>
<div class="paragraph">
<p>Many systems that don’t use an event-sourced model nevertheless rely on immutability: various databases internally use immutable data structures or multi-version data to support point-in-time snapshots. Version control systems such as Git, Mercurial, and Fossil also rely on immutable data to preserve version history of files.</p>
</div>
<div class="paragraph">
<p>To what extent is it feasible to keep an immutable history of all changes forever? The answer depends on the amount of churn in the dataset. Some workloads mostly add data and rarely update or delete; they are easy to make immutable. Other workloads have a high rate of updates and deletes on a comparatively small dataset; in these cases, the immutable history may grow prohibitively large, fragmentation may become an issue, and the performance of compaction and garbage collection becomes crucial for operational robustness.</p>
</div>
<div class="paragraph">
<p>Besides the performance reasons, there may also be circumstances in which you need data to be deleted for administrative reasons, in spite of all immutability. For example, privacy regulations may require deleting a user’s personal information after they close their account, data protection legislation may require erroneous information to be removed, or an accidental leak of sensitive information may need to be contained.</p>
</div>
<div class="paragraph">
<p>Truly deleting data is surprisingly hard, since copies can live in many places: for example, storage engines, filesystems, and SSDs often write to a new location rather than overwriting in place, and backups are often deliberately immutable to prevent accidental deletion or corruption. Deletion is more a matter of “making it harder to retrieve the data” than actually “making it impossible to retrieve the data.”</p>
</div>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="processing-streams">3. Processing Streams</h2>
<div class="sectionbody">
<div class="olist arabic">
<ol class="arabic">
<li>
<p>You can take the data in the events and write it to a database, cache, search index, or similar storage system, from where it can then be queried by other clients.</p>
</li>
<li>
<p>You can push the events to users in some way, for example by sending email alerts or push notifications, or by streaming the events to a real-time dashboard where they are visualized. In this case, a human is the ultimate consumer of the stream.</p>
</li>
<li>
<p>You can process one or more input streams to produce one or more output streams. Streams may go through a pipeline consisting of several such processing stages before they eventually end up at an output (option 1 or 2).</p>
</li>
</ol>
</div>
<div class="paragraph">
<p>A piece of code that processes streams like this is known as an <strong>operator</strong> or a <strong>job</strong>. It is closely related to the Unix processes and MapReduce jobs, and the pattern of <strong>dataflow</strong> is similar: a stream processor consumes input streams in a read-only fashion and writes its output to a different location in an append-only fashion.</p>
</div>
<div class="sect2">
<h3 id="uses-of-stream-processing">3.1. Uses of Stream Processing</h3>
<div class="paragraph">
<p>Stream processing has long been used for monitoring purposes, where an organization wants to be alerted if certain things happen.</p>
</div>
<div class="sect3">
<h4 id="complex-event-processing">3.1.1. Complex event processing</h4>
<div class="paragraph">
<p><strong>Complex event processing</strong> (CEP) is an approach developed in the 1990s for analyzing event streams, especially geared toward the kind of application that requires searching for certain event patterns. Similarly to the way that a regular expression allows you to search for certain patterns of characters in a string, CEP allows you to specify rules to search for certain patterns of events in a stream.</p>
</div>
<div class="paragraph">
<p>CEP systems often use a high-level declarative query language like SQL, or a graphical user interface, to describe the patterns of events that should be detected. These queries are submitted to a processing engine that consumes the input streams and internally maintains a state machine that performs the required matching. When a match is found, the engine emits a <strong>complex event</strong> (hence the name) with the details of the event pattern that was detected.</p>
</div>
<div class="paragraph">
<p>In these systems, the relationship between queries and data is reversed compared to normal databases. Usually, a database stores data persistently and treats queries as transient: when a query comes in, the database searches for data matching the query, and then forgets about the query when it has finished. CEP engines reverse these roles: queries are stored long-term, and events from the input streams continuously flow past them in search of a query that matches an event pattern.</p>
</div>
</div>
<div class="sect3">
<h4 id="stream-analytics">3.1.2. Stream analytics</h4>
<div class="paragraph">
<p>The boundary between CEP and <strong>stream analytics</strong> is blurry, but as a general rule, analytics tends to be less interested in finding specific event sequences and is more oriented toward <strong>aggregations</strong> and <strong>statistical</strong> metrics over a large number of events—for example:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>Measuring the rate of some type of event (how often it occurs per <strong>time interval</strong>)</p>
</li>
<li>
<p>Calculating the rolling average of a value over some <strong>time period</strong></p>
</li>
<li>
<p>Comparing current statistics to previous <strong>time intervals</strong> (e.g., to detect trends or to alert on metrics that are unusually high or low compared to the same time last week)</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>Such statistics are usually computed over fixed time intervals—for example, you might want to know the average number of queries per second to a service over the last 5 minutes, and their 99th percentile response time during that period. Averaging over a few minutes smoothes out irrelevant fluctuations from one second to the next, while still giving you a timely picture of any changes in traffic pattern. The time interval over which you aggregate is known as a <strong>window</strong>.</p>
</div>
<div class="paragraph">
<p>Stream analytics systems sometimes use <strong>probabilistic algorithms</strong>, such as Bloom filters for set membership, HyperLogLog for cardinality estimation, and various percentile estimation algorithms. Probabilistic algorithms produce approximate results, but have the advantage of requiring significantly less memory in the stream processor than exact algorithms. This use of approximation algorithms sometimes leads people to believe that stream processing systems are always lossy and inexact, but that is wrong: there is nothing inherently approximate about stream processing, and probabilistic algorithms are merely an optimization.</p>
</div>
<div class="paragraph">
<p>Many open source distributed stream processing frameworks are designed with analytics in mind: for example, Apache <strong>Storm</strong>, <strong>Spark Streaming</strong>, <strong>Flink</strong>, Concord, Samza, and <strong>Kafka Streams</strong>. Hosted services include Google Cloud Dataflow and Azure Stream Analytics.</p>
</div>
</div>
<div class="sect3">
<h4 id="reasoning-about-time">3.1.3. Reasoning About Time</h4>
<div class="paragraph">
<p>Stream processors often need to deal with time, especially when used for analytics purposes, which frequently use time windows such as “the average over the last five minutes.” It might seem that the meaning of “the last five minutes” should be unambiguous and clear, but unfortunately the notion is surprisingly tricky.</p>
</div>
<div class="sect4">
<h5 id="event-time-versus-processing-time">Event time versus processing time</h5>
<div class="paragraph">
<p>There are many reasons why processing may be delayed: queueing, network faults, a performance issue leading to contention in the message broker or processor, a restart of the stream consumer, or reprocessing of past events while recovering from a fault or after fixing a bug in the code.</p>
</div>
<div class="paragraph">
<p>Moreover, message delays can also lead to unpredictable ordering of messages. For example, say a user first makes one web request (which is handled by web server A), and then a second request (which is handled by server B). A and B emit events describing the requests they handled, but B’s event reaches the message broker before A’s event does. Now stream processors will first see the B event and then the A event, even though they actually occurred in the opposite order.</p>
</div>
<div class="imageblock">
<div class="content">
<img src="/assets/ddia/stream-processing/Figure_11-7_window_event_process_time.png" alt="Figure 11 7 window event process time" width="75%" height="75%">
</div>
</div>
</div>
</div>
<div class="sect3">
<h4 id="types-of-windows">3.1.4. Types of windows</h4>
<div class="paragraph">
<p>Once you know how the timestamp of an event should be determined, the next step is to decide how windows over time periods should be defined. The window can then be used for aggregations, for example to count events, or to calculate the average of values within the window. Several types of windows are in common use:</p>
</div>
<div class="ulist">
<ul>
<li>
<p><strong>Tumbling window</strong></p>
<div class="paragraph">
<p>A tumbling window has a fixed length, and every event belongs to exactly one window. For example, if you have a 1-minute tumbling window, all the events with timestamps between 10:03:00 and 10:03:59 are grouped into one window, events between 10:04:00 and 10:04:59 into the next window, and so on. You could implement a 1-minute tumbling window by taking each event timestamp and rounding it down to the nearest minute to determine the window that it belongs to.</p>
</div>
</li>
<li>
<p><strong>Hopping window</strong></p>
<div class="paragraph">
<p>A hopping window also has a fixed length, but allows windows to overlap in order to provide some smoothing. For example, a 5-minute window with a hop size of 1 minute would contain the events between 10:03:00 and 10:07:59, then the next window would cover events between 10:04:00 and 10:08:59, and so on. You can implement this hopping window by first calculating 1-minute tumbling windows, and then aggregating over several adjacent windows.</p>
</div>
</li>
<li>
<p><strong>Sliding window</strong></p>
<div class="paragraph">
<p>A sliding window contains all the events that occur within some interval of each other. For example, a 5-minute sliding window would cover events at 10:03:39 and 10:08:12, because they are less than 5 minutes apart (note that tumbling and hopping 5-minute windows would not have put these two events in the same window, as they use fixed boundaries). A sliding window can be implemented by keeping a buffer of events sorted by time and removing old events when they expire from the window.</p>
</div>
</li>
<li>
<p><strong>Session window</strong></p>
<div class="paragraph">
<p>Unlike the other window types, a session window has no fixed duration. Instead, it is defined by grouping together all events for the same user that occur closely together in time, and the window ends when the user has been inactive for some time (for example, if there have been no events for 30 minutes). Sessionization is a common requirement for website analytics.</p>
</div>
</li>
</ul>
</div>
</div>
</div>
<div class="sect2">
<h3 id="stream-joins">3.2. Stream Joins</h3>
<div class="ulist">
<ul>
<li>
<p>Stream-stream join (window join)</p>
<div class="paragraph">
<p>Both input streams consist of activity events, and the join operator searches for related events that occur within some window of time. For example, it may match two actions taken by the same user within 30 minutes of each other. The two join inputs may in fact be the same stream (a self-join) if you want to find related events within that one stream.</p>
</div>
</li>
<li>
<p>Stream-table join (stream enrichment)</p>
<div class="paragraph">
<p>One input stream consists of activity events, while the other is a database changelog. The changelog keeps a local copy of the database up to date. For each activity event, the join operator queries the database and outputs an enriched activity event.</p>
</div>
</li>
<li>
<p>Table-table join (materialized view maintenance)</p>
<div class="paragraph">
<p>Both input streams are database changelogs. In this case, every change on one side is joined with the latest state of the other side. The result is a stream of changes to the materialized view of the join between the two tables.</p>
</div>
</li>
</ul>
</div>
</div>
<div class="sect2">
<h3 id="fault-tolerance">3.3. Fault Tolerance</h3>
<div class="sect3">
<h4 id="microbatching-and-checkpointing">3.3.1. Microbatching and checkpointing</h4>
<div class="paragraph">
<p>One solution is to break the stream into small blocks, and treat each block like a miniature batch process. This approach is called <strong>microbatching</strong>, and it is used in Spark Streaming. The batch size is typically around one second, which is the result of a performance compromise: smaller batches incur greater scheduling and coordination overhead, while larger batches mean a longer delay before results of the stream processor become visible.</p>
</div>
<div class="paragraph">
<p>Microbatching also implicitly provides a tumbling window equal to the batch size (windowed by processing time, not event timestamps); any jobs that require larger windows need to explicitly carry over state from one microbatch to the next.</p>
</div>
<div class="paragraph">
<p>A variant approach, used in Apache Flink, is to periodically generate rolling <strong>checkpoints</strong> of state and write them to durable storage. If a stream operator crashes, it can restart from its most recent checkpoint and discard any output generated between the last checkpoint and the crash. The checkpoints are triggered by barriers in the message stream, similar to the boundaries between microbatches, but without forcing a particular window size.</p>
</div>
<div class="paragraph">
<p>Within the confines of the stream processing framework, the microbatching and checkpointing approaches provide the same <strong>exactly-once</strong> semantics as batch processing. However, as soon as output leaves the stream processor (for example, by writing to a database, sending messages to an external message broker, or sending emails), the framework is no longer able to discard the output of a failed batch. In this case, restarting a failed task causes the external <strong>side effect</strong> to happen twice, and microbatching or checkpointing alone is not sufficient to prevent this problem.</p>
</div>
</div>
<div class="sect3">
<h4 id="atomic-commit-revisited">3.3.2. Atomic commit revisited</h4>
<div class="paragraph">
<p>In order to give the appearance of exactly-once processing in the presence of faults, we need to ensure that all outputs and side effects of processing an event take effect if and only if the processing is successful. Those effects include any messages sent to downstream operators or external messaging systems (including email or push notifications), any database writes, any changes to operator state, and any acknowledgment of input messages (including moving the consumer offset forward in a log-based message broker).</p>
</div>
<div class="paragraph">
<p>Those things either all need to happen atomically, or none of them must happen, but they should not go out of sync with each other.</p>
</div>
<div class="ulist">
<ul>
<li>
<p>Distributed transactions vs Log-based derived data systems</p>
<div class="paragraph">
<p>At an abstract level, they achieve a similar goal by different means.</p>
</div>
<div class="paragraph">
<p>Distributed transactions decide on an ordering of writes by using locks for mutual exclusion (2PL), while CDC and event sourcing use a log for ordering.</p>
</div>
<div class="paragraph">
<p>Distributed transactions use atomic commit to ensure that changes take effect exactly once, while log-based systems are often based on deterministic retry and idempotence.</p>
</div>
<div class="paragraph">
<p>The biggest difference is that transaction systems usually provide linearizability, which implies useful guarantees such as reading your own writes.</p>
</div>
<div class="paragraph">
<p>On the other hand, derived data systems are often updated asynchronously, and so they do not by default offer the same timing guarantees.</p>
</div>
</li>
</ul>
</div>
</div>
<div class="sect3">
<h4 id="idempotence">3.3.3. Idempotence</h4>
<div class="paragraph">
<p>Our goal is to discard the partial output of any failed tasks so that they can be safely retried without taking effect twice. Distributed transactions are one way of achieving that goal, but another way is to rely on <strong>idempotence</strong>.</p>
</div>
</div>
<div class="sect3">
<h4 id="rebuilding-state-after-a-failure">3.3.4. Rebuilding state after a failure</h4>
<div class="paragraph">
<p>Any stream process that requires state—for example, any windowed aggregations (such as counters, averages, and histograms) and any tables and indexes used for joins—must ensure that this state can be recovered after a failure.</p>
</div>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="what-is-apache-kafka">4. What Is Apache Kafka?</h2>
<div class="sectionbody">
<div class="paragraph">
<p>Apache Kafka is an event streaming platform used to collect, process, store, and integrate data at scale. It has numerous use cases including distributed streaming, stream processing, data integration, and pub/sub messaging.</p>
</div>
<div class="imageblock">
<div class="content">
<img src="https://images.contentful.com/gt6dp23g0g38/53UO4964r0e7kRVm0mcUUZ/f6f6d7b1b90e8e88a5be0d1845bdf950/what_is_kafka_and_how_does_it_work.png" alt="what is kafka and how does it work" width="75%" height="75%">
</div>
</div>
<div class="paragraph">
<p>An <strong>event</strong> records the fact that "something happened" in the world or in your business. It is also called record or message. For example, a payment, a website click, or a temperature reading, along with a description of what happened. Conceptually, an event has a key, value, timestamp, and optional metadata headers.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="text">key: "Alice"
value: "Made a payment of $200 to Bob"
timestamp: "Jun. 25, 2020 at 2:06 p.m."</code></pre>
</div>
</div>
<div class="paragraph">
<p><strong>Event is usually fairly small, say less than a megabyte</strong> or so, and is normally represented in some structured format, say in JSON or an object serialized with Apache Avro™ or Protocol Buffers.</p>
</div>
<div class="paragraph">
<p>Kafka is based on the abstraction of a distributed <strong>commit log</strong>. By splitting a log into <strong>partitions</strong>, Kafka is able to scale-out systems.</p>
</div>
<div class="paragraph">
<p><strong>Producers</strong> are those client applications that <strong>publish</strong> (write) events to Kafka, and <strong>consumers</strong> are those that <strong>subscribe</strong> to (read and process) these events.</p>
</div>
<div class="paragraph">
<p>A <strong>topic</strong> is a log of events. Logs are easy to understand, because they are simple data structures with well-known semantics.</p>
</div>
<div class="ulist">
<ul>
<li>
<p>First, they are <strong>append only</strong>: When you write a new message into a log, it always goes on the end.</p>
</li>
<li>
<p>Second, they can only be read by seeking an arbitrary <strong>offset</strong> in the log, then by scanning sequential log entries.</p>
</li>
<li>
<p>Third, events in the log are <strong>immutable</strong>—once something has happened, it is exceedingly difficult to make it un-happen.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>Kafka is composed of a network of machines called <strong>brokers</strong>. Each broker hosts some set of partitions and handles incoming requests to write new events to those partitions or read events from them.</p>
</div>
<div class="paragraph">
<p>Topics are <strong>partitioned</strong>, meaning a topic is spread over a number of "buckets" located on different Kafka brokers.</p>
</div>
<div class="paragraph">
<p><span class="image"><img src="https://kafka.apache.org/images/streams-and-tables-p1_p4.png" alt="streams and tables p1 p4" width="55%" height="55%"></span></p>
</div>
<div class="paragraph">
<p>Having broken a topic up into partitions, we need a way of deciding which messages to write to which partitions.</p>
</div>
<div class="ulist">
<ul>
<li>
<p>If a message has no key, subsequent messages will be distributed round-robin among all the topic’s partitions.</p>
<div class="paragraph">
<p>In this case, all partitions get an even share of the data, but we don’t preserve any kind of ordering of the input messages.</p>
</div>
</li>
<li>
<p>If the message does have a key, then the destination partition will be computed from a hash of the key.</p>
<div class="paragraph">
<p>This allows Kafka to guarantee that messages having the same key always land in the same partition, and therefore are always in order.</p>
</div>
</li>
</ul>
</div>
<div class="paragraph">
<p>To make your data fault-tolerant and highly-available, the partitions of every <strong>topic can be replicated</strong>. A common production setting is a replication factor of 3, i.e., there will always be three copies of your data.</p>
</div>
<div class="sect2">
<h3 id="the-producer">4.1. The Producer</h3>
<div class="ulist">
<ul>
<li>
<p>The <strong>producer sends data directly to the broker</strong> that is the leader for the partition without any intervening routing tier.</p>
<div class="paragraph">
<p>To help the producer do this all Kafka nodes can answer a request for metadata about which servers are alive and where the leaders for the partitions of a topic are at any given time to allow the producer to appropriately direct its requests.</p>
</div>
</li>
<li>
<p>The <strong>client controls which partition it publishes messages to</strong>. This can be done at random, implementing a kind of <strong>random load balancing</strong>, or it can be done by some <strong>semantic partitioning</strong> function.</p>
<div class="paragraph">
<p>We expose the interface for semantic partitioning by allowing the user to specify a key to partition by and using this to hash to a partition.</p>
</div>
</li>
<li>
<p>Batching is one of the big drivers of efficiency, and to enable batching the Kafka producer will attempt to accumulate data in memory and to send out larger batches in a single request asynchronously.</p>
</li>
</ul>
</div>
</div>
<div class="sect2">
<h3 id="the-consumer">4.2. The Consumer</h3>
<div class="paragraph">
<p>Messaging traditionally has two models: <a href="http://en.wikipedia.org/wiki/Message_queue">queuing</a> and <a href="http://en.wikipedia.org/wiki/Publish%E2%80%93subscribe_pattern">publish-subscribe</a>.</p>
</div>
<div class="ulist">
<ul>
<li>
<p>In a queue, a pool of consumers may read from a server and each message goes to one of them;</p>
</li>
<li>
<p>In publish-subscribe the message is broadcast to all consumers.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>Kafka offers a single consumer abstraction that generalizes both of these—the <strong>consumer group</strong>.</p>
</div>
<div class="imageblock">
<div class="content">
<img src="https://kafka.apache.org/images/consumer-groups.png" alt="consumer groups" width="45%" height="45%">
</div>
<div class="title">Figure 1. A two server Kafka cluster hosting four partitions (P0-P3) with two consumer groups. Consumer group A has two consumer instances and group B has four.</div>
</div>
<div class="paragraph">
<p>Consumers label themselves with a consumer <strong>group name</strong>, and each message published to a topic is delivered to one consumer instance within each subscribing consumer group. Consumer instances can be in separate processes or on separate machines.</p>
</div>
<div class="paragraph">
<p>If all the consumer instances have the same consumer group, then this works just like a traditional queue balancing load over the consumers.</p>
</div>
<div class="paragraph">
<p>If all the consumer instances have different consumer groups, then this works like publish-subscribe and all messages are broadcast to all consumers.</p>
</div>
<div class="paragraph">
<p>Kafka has stronger ordering guarantees than a traditional messaging system, too.</p>
</div>
<div class="paragraph">
<p>A traditional queue retains messages in-order on the server, and if multiple consumers consume from the queue then the server hands out messages in the order they are stored.</p>
</div>
<div class="ulist">
<ul>
<li>
<p>However, although the server hands out messages in order, the messages are delivered asynchronously to consumers, so they may arrive out of order on different consumers.</p>
</li>
<li>
<p>This effectively means the ordering of the messages is lost in the presence of parallel consumption.</p>
</li>
<li>
<p>Messaging systems often work around this by having a notion of "exclusive consumer" that allows only one process to consume from a queue, but of course this means that there is no parallelism in processing.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>Kafka does it better. By having a notion of parallelism—the partition—within the topics, Kafka is able to provide both ordering guarantees and load balancing over a pool of consumer processes.</p>
</div>
<div class="ulist">
<ul>
<li>
<p>This is achieved by assigning the partitions in the topic to the consumers in the consumer group so that <strong>each partition is consumed by exactly one consumer in the group</strong>.</p>
</li>
<li>
<p>By doing this we ensure that the consumer is the only reader of that partition and consumes the data in order.</p>
</li>
<li>
<p>Since there are many partitions this still balances the load over many consumer instances.</p>
</li>
<li>
<p>Note however that there cannot be more consumer instances than partitions.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p><strong>Kafka only provides a total order over messages within a partition.</strong></p>
</div>
<div class="ulist">
<ul>
<li>
<p>This combined with the ability to partition data by key is sufficient for the vast majority of applications.</p>
</li>
<li>
<p>However, if you require <strong>a total order over messages this can be achieved with a topic that has only one partition, though this will mean only one consumer process.</strong></p>
</li>
</ul>
</div>
<div class="paragraph">
<p>Push vs. pull</p>
</div>
<div class="ulist">
<ul>
<li>
<p>The Kafka consumer works by issuing "fetch" requests to the brokers leading the partitions it wants to consume.</p>
<div class="paragraph">
<p>The <strong>consumer specifies its offset in the log</strong> with each request and receives back a chunk of log beginning from that position.</p>
</div>
<div class="paragraph">
<p>The consumer thus has significant control over this position and can <strong>rewind</strong> it to re-consume data if need be.</p>
</div>
</li>
</ul>
</div>
</div>
<div class="sect2">
<h3 id="message-delivery-semantics">4.3. Message Delivery Semantics</h3>
<div class="paragraph">
<p>In a distributed publish-subscribe messaging system, the computers that make up the system can always fail independently of one another. In the case of Kafka, an individual broker can crash, or a network failure can happen while the producer is sending a message to a topic. Depending on the action the producer takes to handle such a failure, you can get different semantics:</p>
</div>
<div class="ulist">
<ul>
<li>
<p><strong>At-least-once</strong> semantics:</p>
<div class="paragraph">
<p>If the producer receives an acknowledgement (<strong>ack</strong>) from the Kafka broker and <em>acks=all</em>, it means that the message has been written exactly once to the Kafka topic.</p>
</div>
<div class="paragraph">
<p>However, if a producer ack times out or receives an error, it might <strong>retry</strong> sending the message assuming that the message was not written to the Kafka topic.</p>
</div>
<div class="paragraph">
<p>If the broker had failed right before it sent the ack but after the message was successfully written to the Kafka topic, this retry leads to the message being written twice and hence delivered more than once to the end consumer. And everybody loves a cheerful giver, but this approach can lead to duplicated work and incorrect results.</p>
</div>
</li>
<li>
<p><strong>At-most-once</strong> semantics:</p>
<div class="paragraph">
<p>If the producer does <strong>not retry</strong> when an ack times out or returns an error, then the message might end up not being written to the Kafka topic, and hence not delivered to the consumer.</p>
</div>
<div class="paragraph">
<p>In most cases it will be, but in order to avoid the possibility of duplication, we accept that sometimes messages will not get through.</p>
</div>
</li>
<li>
<p><strong>Exactly-once</strong> semantics:</p>
<div class="paragraph">
<p>Even if a producer retries sending a message, it leads to the message being delivered exactly once to the end consumer.</p>
</div>
<div class="paragraph">
<p>Exactly-once semantics is the most desirable guarantee, but also a poorly understood one. Because it requires a <strong>cooperation</strong> between the messaging system itself and the application producing and consuming the messages.</p>
</div>
<div class="paragraph">
<p>For instance, if after consuming a message successfully you rewind your Kafka consumer to a previous offset, you will receive all the messages from that offset to the latest one, all over again. This shows why the messaging system and the client application must cooperate to make exactly-once semantics happen.</p>
</div>
</li>
</ul>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="references">5. References</h2>
<div class="sectionbody">
<div class="ulist">
<ul>
<li>
<p>Martin Kleppmann: Designing Data-Intensive Applications, O’Reilly, 2017.</p>
</li>
<li>
<p>What Is Apache Kafka?, <a href="https://developer.confluent.io/what-is-apache-kafka/" class="bare">https://developer.confluent.io/what-is-apache-kafka/</a></p>
</li>
<li>
<p>Apache Kafka, <a href="https://kafka.apache.org/documentation/" class="bare">https://kafka.apache.org/documentation/</a></p>
</li>
<li>
<p>Apache Kafka, <a href="https://kafka.apache.org/08/documentation.html" class="bare">https://kafka.apache.org/08/documentation.html</a></p>
</li>
<li>
<p>Exactly-Once Semantics Are Possible: Here’s How Kafka Does It, <a href="https://www.confluent.io/blog/exactly-once-semantics-are-possible-heres-how-apache-kafka-does-it/" class="bare">https://www.confluent.io/blog/exactly-once-semantics-are-possible-heres-how-apache-kafka-does-it/</a></p>
</li>
</ul>
</div>
</div>
</div>
  </div>

  <ul class="post-navigation">
    <li>
      
      <a href="/2022/08/09/consistency-and-consensus/">&laquo; Consistency and Consensus</a>
      
    </li>
    <li>
      
      <a href="/2022/08/24/the-master-method-for-solving-recurrences/">The master method for solving recurrences &raquo;</a>
      
    </li>
  </ul>
</article>

      </div>
    </div>

    <footer class="site-footer">
  <div class="license">
    <span>Article licensed under <a href="http://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a></span>
  </div>
  
  <details open>
    <summary>Extral Links</summary>
    <div>
      
      <a href="https://jekyllrb.com/">Jekyll</a>
      
      &nbsp;.&nbsp;
      
      
      <a href="https://shopify.github.io/">Liquid</a>
      
      &nbsp;.&nbsp;
      
      
      <a href="https://docs.asciidoctor.org/">Asciidoctor</a>
      
      &nbsp;.&nbsp;
      
      
      <a href="https://github.com/qqbuby/">GitHub</a>
      
      &nbsp;.&nbsp;
      
      
      <a href="/feed.xml">RSS</a>
      
      
    </div>
  </details>
  
</footer>


<!-- https://github.com/bryanbraun/anchorjs -->
<script src="/js/anchor.min.js"></script>
<script>
  anchors.add();
  anchors.remove(".site-title");
</script>




  </body>

</html>

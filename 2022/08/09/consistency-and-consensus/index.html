<!DOCTYPE html>
<html>

  <head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <!-- Bing WebMaster -->
  <meta name="msvalidate.01" content="AB2FFF876C37F59D9121882CC8395DE5" />

  <title>Consistency and Consensus</title>
  <meta name="description" content="">

  <link rel="stylesheet" href="/css/main.css">
  <link rel="canonical" href="https://blog.codefarm.me/2022/08/09/consistency-and-consensus/">
  <link rel="alternate" type="application/rss+xml" title="CODE FARM" href="https://blog.codefarm.me/feed.xml">

  <!-- https://cdn.jsdelivr.net/gh/lurongkai/anti-baidu/js/anti-baidu-latest.min.js -->
<script type="text/javascript" src="/js/anti-baidu.min.js" charset="UTF-8"></script>

  
<!-- Google Analytics Website tracking -->
<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-83971182-1', 'auto');
  ga('send', 'pageview');

</script>


  
<!-- Google tag (gtag.js) -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-SN88FJ18E5"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-SN88FJ18E5');
</script>



</head>


  <body>

    <header class="site-header">

  <div class="wrapper">
    <h2 class="site-title">
      <a class="site-title" href="/">CODE FARM</a>
    </h2>

     <nav class="site-nav">
      <a href="#" class="menu-icon">
        <svg viewBox="0 0 18 15">
          <path fill="#424242" d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.031C17.335,0,18,0.665,18,1.484L18,1.484z"/>
          <path fill="#424242" d="M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0c0-0.82,0.665-1.484,1.484-1.484 h15.031C17.335,6.031,18,6.696,18,7.516L18,7.516z"/>
          <path fill="#424242" d="M18,13.516C18,14.335,17.335,15,16.516,15H1.484C0.665,15,0,14.335,0,13.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.031C17.335,12.031,18,12.696,18,13.516L18,13.516z"/>
        </svg>
      </a>
        <div class="trigger">
            <ul>
                <li><a href="/">home</a>
                <li><a href="/category">category</a>
                <li><a href="/tag">tag</a>
                <li><a href="/archive">archive</a>
                <li><a href="/about">about</a>
                <li><a href="https://resume.github.io/?qqbuby" target="_blank">R&eacute;sum&eacute;</a>
            </ul>
      </div>
    </nav>

  </div>

</header>


    <div class="page-content">
      <div class="wrapper">
        <article class="post" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title" itemprop="name headline">Consistency and Consensus</h1>
    
    
    <p class="post-meta"><time datetime="2022-08-09T09:48:14+08:00" itemprop="datePublished">Aug 9, 2022</time></p>
  </header>

  <div class="post-content" itemprop="articleBody">
    <div id="toc" class="toc">
<div id="toctitle"></div>
<ul class="sectlevel1">
<li><a href="#consistency-guarantees">1. Consistency Guarantees</a></li>
<li><a href="#linearizability">2. Linearizability</a>
<ul class="sectlevel2">
<li><a href="#what-makes-a-system-linearizable">2.1. What Makes a System Linearizable?</a></li>
<li><a href="#relying-on-linearizability">2.2. Relying on Linearizability</a></li>
<li><a href="#implementing-linearizable-systems">2.3. Implementing Linearizable Systems</a></li>
<li><a href="#the-cost-of-linearizability">2.4. The Cost of Linearizability</a>
<ul class="sectlevel3">
<li><a href="#the-cap-theorem">2.4.1. The CAP theorem</a></li>
<li><a href="#linearizability-and-network-delays">2.4.2. Linearizability and network delays</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#ordering-guarantees">3. Ordering Guarantees</a>
<ul class="sectlevel2">
<li><a href="#ordering-and-causality">3.1. Ordering and Causality</a>
<ul class="sectlevel3">
<li><a href="#the-causal-order-is-not-a-total-order">3.1.1. The causal order is not a total order</a></li>
<li><a href="#linearizability-is-stronger-than-causal-consistency">3.1.2. Linearizability is stronger than causal consistency</a></li>
<li><a href="#capturing-causal-dependencies">3.1.3. Capturing causal dependencies</a></li>
</ul>
</li>
<li><a href="#sequence-number-ordering">3.2. Sequence Number Ordering</a>
<ul class="sectlevel3">
<li><a href="#noncausal-sequence-number-generators">3.2.1. Noncausal sequence number generators</a></li>
<li><a href="#lamport-timestamps">3.2.2. Lamport timestamps</a></li>
<li><a href="#timestamp-ordering-is-not-sufficient">3.2.3. Timestamp ordering is not sufficient</a></li>
</ul>
</li>
<li><a href="#total-order-broadcast">3.3. Total Order Broadcast</a>
<ul class="sectlevel3">
<li><a href="#using-total-order-broadcast">3.3.1. Using total order broadcast</a></li>
<li><a href="#implementing-linearizable-storage-using-total-order-broadcast">3.3.2. Implementing linearizable storage using total order broadcast</a></li>
<li><a href="#implementing-total-order-broadcast-using-linearizable-storage">3.3.3. Implementing total order broadcast using linearizable storage</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#distributed-transactions-and-consensus">4. Distributed Transactions and Consensus</a>
<ul class="sectlevel2">
<li><a href="#atomic-commit-and-two-phase-commit-2pc">4.1. Atomic Commit and Two-Phase Commit (2PC)</a>
<ul class="sectlevel3">
<li><a href="#from-single-node-to-distributed-atomic-commit">4.1.1. From single-node to distributed atomic commit</a></li>
<li><a href="#introduction-to-two-phase-commit">4.1.2. Introduction to two-phase commit</a></li>
<li><a href="#distributed-transactions-in-practice">4.1.3. Distributed Transactions in Practice</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#fault-tolerant-consensus">5. Fault-Tolerant Consensus</a>
<ul class="sectlevel2">
<li><a href="#consensus-algorithms-and-total-order-broadcast">5.1. Consensus algorithms and total order broadcast</a>
<ul class="sectlevel3">
<li><a href="#limitations-of-consensus">5.1.1. Limitations of consensus</a></li>
</ul>
</li>
<li><a href="#membership-and-coordination-services">5.2. Membership and Coordination Services</a></li>
</ul>
</li>
<li><a href="#references">6. References</a></li>
</ul>
</div>
<div id="preamble">
<div class="sectionbody">
<div class="paragraph">
<p>The simplest way of handling faults in distributed systems is to simply let the entire service fail, and show the user an error message. If that solution is unacceptable, we need to find ways of <strong>tolerating</strong> faults—that is, of keeping the service functioning correctly, even if some internal component is faulty.</p>
</div>
<div class="paragraph">
<p>The best way of building <strong>fault-tolerant</strong> systems is to find some general-purpose abstractions with useful guarantees, implement them once, and then let applications rely on those guarantees. This is the same approach as we used with transactions: by using a transaction, the application can pretend that there are no crashes (<strong>atomicity</strong>), that nobody else is concurrently accessing the database (<strong>isolation</strong>), and that storage devices are perfectly reliable (<strong>durability</strong>). Even though crashes, race conditions, and disk failures do occur, the transaction abstraction hides those problems so that the application doesn’t need to worry about them.</p>
</div>
<div class="paragraph">
<p>One of the most important abstractions for distributed systems is <strong>consensus</strong>: that is, getting all of the nodes to agree on something.</p>
</div>
</div>
</div>
<div class="sect1">
<h2 id="consistency-guarantees">1. Consistency Guarantees</h2>
<div class="sectionbody">
<div class="paragraph">
<p>If you look at two nodes in a replicated database at the same moment in time, you’re likely to see different data on the two nodes, because write requests arrive on different nodes at different times. These inconsistencies occur no matter what replication method the database uses (single-leader, multi-leader, or leaderless replication).</p>
</div>
<div class="paragraph">
<p>Most replicated databases provide at least <strong>eventual consistency</strong>, which means that if you stop writing to the database and wait for some unspecified length of time, then eventually all read requests will return the same value. In other words, the inconsistency is temporary, and it eventually resolves itself (assuming that any faults in the network are also eventually repaired). A better name for eventual consistency may be <strong>convergence</strong>, as we expect all replicas to eventually converge to the same value.</p>
</div>
<div class="paragraph">
<p>However, this is a very <strong>weak</strong> guarantee—it doesn’t say anything about <strong>when</strong> the replicas will converge. Until the time of convergence, reads could return anything or nothing.</p>
</div>
<div class="paragraph">
<p>But, <strong>stronger consistency models</strong> don’t come for free: systems with stronger guarantees may have <strong>worse performance</strong> or be <strong>less fault-tolerant</strong> than systems with weaker guarantees.</p>
</div>
</div>
</div>
<div class="sect1">
<h2 id="linearizability">2. Linearizability</h2>
<div class="sectionbody">
<div class="paragraph">
<p>In an eventually consistent database, if you ask two different replicas the same question at the same time, you may get two different answers. That’s confusing.</p>
</div>
<div class="paragraph">
<p>The idea behind <strong>linearizability</strong> (also known as <strong>atomic consistency</strong>, <strong>strong consistency</strong>, <strong>immediate consistency</strong>, or <strong>external consistency</strong> is to make a system appear as if there were only one copy of the data, and all operations on it are atomic. With this guarantee, even though there may be multiple replicas in reality, the application does not need to worry about them.</p>
</div>
<div class="paragraph">
<p>In a linearizable system, as soon as one client successfully completes a write, all clients reading from the database must be able to see the value just written. Maintaining the illusion of a single copy of the data means guaranteeing that the value read is the most recent, up-to-date value, and doesn’t come from a stale cache or replica. In other words, linearizability is a <strong>recency guarantee</strong>.</p>
</div>
<div class="imageblock">
<div class="content">
<img src="/assets/ddia/consistency-and-consensus/Figure_9-1_system_not_linearizability.png" alt="Figure 9 1 system not linearizability" width="75%" height="75%">
</div>
</div>
<div class="sect2">
<h3 id="what-makes-a-system-linearizable">2.1. What Makes a System Linearizable?</h3>
<div class="paragraph">
<p>The basic idea behind linearizability is simple: to make a system appear as if there is <strong>only a single copy of the data</strong>.</p>
</div>
<div class="paragraph">
<p>Figure 9-2 shows three clients concurrently reading and writing the same key <em>x</em> in a linearizable database. In the distributed systems literature, <em>x</em> is called a <strong>register</strong>—in practice, it could be one key in a key-value store, one row in a relational database, or one document in a document database, for example.</p>
</div>
<div class="imageblock">
<div class="content">
<img src="/assets/ddia/consistency-and-consensus/Figure_9-2_read_concurrent_with_a_write.png" alt="Figure 9 2 read concurrent with a write" width="75%" height="75%">
</div>
</div>
<div class="paragraph">
<p>For simplicity, Figure 9-2 shows only the requests from the clients’ point of view, not the internals of the database. Each bar is a request made by a client, where the start of a bar is the time when the request was sent, and the end of a bar is when the response was received by the client. Due to variable network delays, a client doesn’t know exactly when the database processed its request—it only knows that it must have happened sometime between the client sending the request and receiving the response.</p>
</div>
<div class="paragraph">
<p>In this example, the register has two types of operations:</p>
</div>
<div class="ulist">
<ul>
<li>
<p><code>read(x) ⇒ v</code> means the client requested to read the value of register <em>x</em>, and the database returned the value <em>v</em>.</p>
</li>
<li>
<p><code>write(x, v) ⇒ r</code> means the client requested to set the register <em>x</em> to value <em>v</em>, and the database returned response <em>r</em> (which could be <em>ok</em> or <em>error</em>).</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>In Figure 9-2, the value of <em>x</em> is initially 0, and client C performs a write request to set it to 1. While this is happening, clients A and B are repeatedly polling the database to read the latest value. What are the possible responses that A and B might get for their read requests?</p>
</div>
<div class="ulist">
<ul>
<li>
<p>The first read operation by client A completes before the write begins, so it must definitely return the old value 0.</p>
</li>
<li>
<p>The last read by client A begins after the write has completed, so it must definitely return the new value 1 if the database is linearizable: we know that the write must have been processed sometime between the start and end of the write operation, and the read must have been processed sometime between the start and end of the read operation. If the read started after the write ended, then the read must have been processed after the write, and therefore it must see the new value that was written.</p>
</li>
<li>
<p>Any <em>read operations that overlap in time with the write operation</em> might return either 0 or 1, because we don’t know whether or not the write has taken effect at the time when the read operation is processed. These operations are concurrent with the write.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>However, that is not yet sufficient to fully describe linearizability: if reads that are concurrent with a write can return either the old or the new value, then readers could see a value <strong>flip back and forth</strong> between the old and the new value several times while a write is going on. That is not what we expect of a system that emulates a “single copy of the data.”</p>
</div>
<div class="imageblock">
<div class="content">
<img src="/assets/ddia/consistency-and-consensus/Figure_9-3_read_after_write_new_value.png" alt="Figure 9 3 read after write new value" width="75%" height="75%">
</div>
</div>
<div class="paragraph">
<p>In a linearizable system we imagine that there must be some point in time (between the start and end of the write operation) at which the value of <em>x</em> <strong>atomically flips</strong> from 0 to 1. Thus, if one client’s read returns the new value 1, all subsequent reads must also return the new value, even if the write operation has not yet completed.</p>
</div>
<div class="imageblock">
<div class="content">
<img src="/assets/ddia/consistency-and-consensus/Figure_9-4_visualizing_points_in_time_linearizability.png" alt="Figure 9 4 visualizing points in time linearizability" width="75%" height="75%">
</div>
</div>
<div class="paragraph">
<p>In Figure 9-4 we add a third type of operation besides read and write:</p>
</div>
<div class="ulist">
<ul>
<li>
<p><code>cas(x, vold, vnew) ⇒ r</code> means the client requested an atomic compare-and-set operation. If the current value of the register <em>x</em> equals vold, it should be atomically set to <em>vnew</em>. If <em>x ≠ vold</em> then the operation should leave the register unchanged and return an error. <em>r</em> is the database’s response (<em>ok</em> or <em>error</em>).</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>The requirement of linearizability is that the lines joining up the operation markers always move forward in time (from left to right), never backward. This requirement ensures the recency guarantee: <strong>once a new value has been written or read, all subsequent reads see the value that was written, until it is overwritten again.</strong></p>
</div>
<div class="admonitionblock tip">
<table>
<tr>
<td class="icon">
<i class="fa icon-tip" title="Tip"></i>
</td>
<td class="content">
<div class="title">Linearizability Versus Serializability</div>
<div class="paragraph">
<p>Linearizability is easily confused with serializability, as both words seem to mean something like “can be arranged in a sequential order.” However, they are two quite different guarantees, and it is important to distinguish between them:</p>
</div>
<div class="ulist">
<ul>
<li>
<p><strong>Serializability</strong></p>
<div class="paragraph">
<p>Serializability is an <strong>isolation property of transactions</strong>, where every transaction may read and write multiple objects (rows, documents, records). It guarantees that transactions behave the same as if they had executed in some serial order (each transaction running to completion before the next transaction starts). It is okay for that serial order to be different from the order in which transactions were actually run.</p>
</div>
</li>
<li>
<p><strong>Linearizability</strong></p>
<div class="paragraph">
<p>Linearizability is a <strong>recency guarantee</strong> on reads and writes of a register (an individual object). It doesn’t group operations together into transactions, so it does not prevent problems such as write skew, unless you take additional measures such as materializing conflicts.</p>
</div>
</li>
</ul>
</div>
<div class="paragraph">
<p>A database may provide both serializability and linearizability, and this combination is known as strict serializability or strong one-copy serializability (strong-1SR).</p>
</div>
<div class="paragraph">
<p>Implementations of serializability based on <strong>two-phase locking or actual serial execution are typically linearizable</strong>.</p>
</div>
<div class="paragraph">
<p>However, <strong>serializable snapshot isolation is not linearizable</strong>: by design, it makes reads from a consistent snapshot, to avoid lock contention between readers and writers. The whole point of a consistent snapshot is that it does not include writes that are more recent than the snapshot, and thus reads from the snapshot are not linearizable.</p>
</div>
</td>
</tr>
</table>
</div>
</div>
<div class="sect2">
<h3 id="relying-on-linearizability">2.2. Relying on Linearizability</h3>
<div class="ulist">
<ul>
<li>
<p><strong>Locking and leader election</strong></p>
<div class="paragraph">
<p>A system that uses single-leader replication needs to ensure that there is indeed only one leader, not several (split brain). One way of electing a leader is to use a lock: every node that starts up tries to acquire the lock, and the one that succeeds becomes the leader. No matter how this lock is implemented, it must be linearizable: all nodes must agree which node owns the lock; otherwise it is useless.</p>
</div>
<div class="paragraph">
<p>Coordination services like Apache <strong>ZooKeeper</strong> and <strong>etcd</strong> are often used to implement <strong>distributed locks</strong> and <strong>leader election</strong>. They use <strong>consensus algorithms</strong> to implement <strong>linearizable</strong> operations in a <strong>fault-tolerant</strong> way.</p>
</div>
</li>
<li>
<p><strong>Constraints and uniqueness guarantees</strong></p>
<div class="paragraph">
<p>Uniqueness constraints are common in databases: for example, a username or email address must uniquely identify one user, and in a file storage service there cannot be two files with the same path and filename. If you want to enforce this constraint as the data is written (such that if two people try to concurrently create a user or a file with the same name, one of them will be returned an error), you need linearizability.</p>
</div>
</li>
<li>
<p><strong>Cross-channel timing dependencies</strong></p>
<div class="paragraph">
<p>For example, say you have a website where users can upload a photo, and a background process resizes the photos to lower resolution for faster download (thumbnails). The architecture and dataflow of this system is illustrated in Figure 9-5.</p>
</div>
<div class="paragraph">
<p>The image resizer needs to be explicitly instructed to perform a resizing job, and this instruction is sent from the web server to the resizer via a message queue. The web server doesn’t place the entire photo on the queue, since most message brokers are designed for small messages, and a photo may be several megabytes in size. Instead, the photo is first written to a file storage service, and once the write is complete, the instruction to the resizer is placed on the queue.</p>
</div>
<div class="imageblock">
<div class="content">
<img src="/assets/ddia/consistency-and-consensus/Figure_9-5_web_server_cross_channel_race_condition.png" alt="Figure 9 5 web server cross channel race condition" width="75%" height="75%">
</div>
</div>
<div class="paragraph">
<p>If the file storage service is linearizable, then this system should work fine. If it is not linearizable, there is the risk of a race condition: the message queue (steps 3 and 4 in Figure 9-5) might be faster than the internal replication inside the storage service. In this case, when the resizer fetches the image (step 5), it might see an old version of the image, or nothing at all. If it processes an old version of the image, the full-size and resized images in the file storage become permanently inconsistent.</p>
</div>
<div class="paragraph">
<p>This problem arises because there are <strong>two different communication channels</strong> between the web server and the resizer: the file storage and the message queue. Without the recency guarantee of linearizability, race conditions between these two channels are possible.</p>
</div>
</li>
</ul>
</div>
</div>
<div class="sect2">
<h3 id="implementing-linearizable-systems">2.3. Implementing Linearizable Systems</h3>
<div class="paragraph">
<p>Since linearizability essentially means “behave as though there is <strong>only a single copy of the data, and all operations on it are atomic</strong>,” the simplest answer would be to really only use a single copy of the data. However, that approach would not be able to tolerate faults: if the node holding that one copy failed, the data would be lost, or at least inaccessible until the node was brought up again.</p>
</div>
<div class="paragraph">
<p>The most common approach to making a system fault-tolerant is to use replication.</p>
</div>
<div class="ulist">
<ul>
<li>
<p><strong>Single-leader replication (potentially linearizable)</strong></p>
<div class="paragraph">
<p>In a system with single-leader replication, the leader has the primary copy of the data that is used for writes, and the followers maintain backup copies of the data on other nodes. If you <strong>make reads from the leader</strong>, or from <strong>synchronously updated followers</strong>, they have the potential to be linearizable. However, not every single-leader database is actually linearizable, either by design (e.g., because it uses <strong>snapshot isolation</strong>) or due to <strong>concurrency bugs</strong>.</p>
</div>
<div class="paragraph">
<p>Using the leader for reads relies on the assumption that you know for sure who the leader is. It is quite possible for a node to think that it is the leader, when in fact it is not—and if the delusional leader continues to serve requests, it is likely to violate linearizability. With asynchronous replication, failover may even lose committed writes, which violates both durability and linearizability.</p>
</div>
</li>
<li>
<p><strong>Consensus algorithms (linearizable)</strong></p>
<div class="paragraph">
<p>Some consensus algorithms bear a resemblance to single-leader replication. However, consensus protocols contain measures to prevent split brain and stale replicas. Thanks to these details, consensus algorithms can implement linearizable storage safely. This is how ZooKeeper and etcd work, for example.</p>
</div>
</li>
<li>
<p><strong>Multi-leader replication (not linearizable)</strong></p>
<div class="paragraph">
<p>Systems with multi-leader replication are generally not linearizable, because they concurrently process writes on multiple nodes and asynchronously replicate
them to other nodes. For this reason, they can produce conflicting writes that require resolution. Such conflicts are an artifact of the lack of a single copy of the data.</p>
</div>
</li>
<li>
<p><strong>Leaderless replication (probably not linearizable)</strong></p>
<div class="paragraph">
<p>For systems with leaderless replication (Dynamo-style), people sometimes claim that you can obtain “strong consistency” by requiring quorum reads and writes (w + r &gt; n). Depending on the exact configuration of the quorums, and depending on how you define strong consistency, this is not quite true.</p>
</div>
<div class="paragraph">
<p>“Last write wins” conflict resolution methods based on time-of-day clocks (e.g., in Cassandra) are almost certainly nonlinearizable, because clock timestamps cannot be guaranteed to be consistent with actual event ordering due to clock skew. Sloppy quorums also ruin any chance of linearizability. Even with strict quorums, nonlinearizable behavior is possible.</p>
</div>
<div class="paragraph">
<p>Intuitively, it seems as though strict quorum reads and writes should be linearizable in a Dynamo-style model. However, when we have variable network delays, it is possible to have race conditions.</p>
</div>
<div class="imageblock">
<div class="content">
<img src="/assets/ddia/consistency-and-consensus/Figure_9-6_nonlinearizable_strict_quorum.png" alt="Figure 9 6 nonlinearizable strict quorum" width="75%" height="75%">
</div>
</div>
<div class="paragraph">
<p>In summary, it is safest to assume that a leaderless system with Dynamo-style replication does not provide linearizability.</p>
</div>
</li>
</ul>
</div>
</div>
<div class="sect2">
<h3 id="the-cost-of-linearizability">2.4. The Cost of Linearizability</h3>
<div class="paragraph">
<p><span class="image"><img src="/assets/ddia/consistency-and-consensus/Figure_9-7_network_faults_linearizability_availablity.png" alt="Figure 9 7 network faults linearizability availablity" width="75%" height="75%"></span></p>
</div>
<div class="sect3">
<h4 id="the-cap-theorem">2.4.1. The CAP theorem</h4>
<div class="ulist">
<ul>
<li>
<p>If your application <strong>requires linearizability</strong>, and some replicas are disconnected from the other replicas due to a network problem, then some replicas cannot
process requests while they are disconnected: they must either wait until the network problem is fixed, or return an error (either way, they become <strong>unavailable</strong>).</p>
</li>
<li>
<p>If your application does <strong>not require linearizability</strong>, then it can be written in a way that each replica can process requests independently, even if it is disconnected from other replicas (e.g., multi-leader). In this case, the application can remain <strong>available</strong> in the face of a network problem, but its behavior is <strong>not linearizable</strong>.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>Thus, applications that don’t require linearizability can be more tolerant of network problems. This insight is popularly known as the <strong>CAP theorem</strong>, named by Eric Brewer in 2000, although the trade-off has been known to designers of distributed databases since the 1970s.</p>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
<div class="title">The Unhelpful CAP Theorem</div>
<div class="paragraph">
<p>CAP is sometimes presented as <strong>Consistency</strong>, <strong>Availability</strong>, <strong>Partition tolerance</strong>: pick 2 out of 3. Unfortunately, putting it this way is misleading because network partitions are a kind of fault, so they aren’t something about which you have a choice: they will happen whether you like it or not.</p>
</div>
<div class="paragraph">
<p>At times when the network is working correctly, a system can provide both consistency (linearizability) and total availability. When a network fault occurs, you have to choose between either linearizability or total availability. Thus, a better way of phrasing CAP would be either Consistent or Available when Partitioned. A more reliable network needs to make this choice less often, but at some point the choice is inevitable.</p>
</div>
<div class="paragraph">
<p>In discussions of CAP there are several contradictory definitions of the term availability, and the formalization as a theorem does not match its usual meaning. Many so-called “highly available” (fault-tolerant) systems actually do not meet CAP’s idiosyncratic definition of availability. All in all, there is a lot of misunderstanding and confusion around CAP, and it does not help us understand systems better, so CAP is best avoided.</p>
</div>
</td>
</tr>
</table>
</div>
<div class="paragraph">
<p>The CAP theorem as formally defined [30] is of very narrow scope: it only considers one consistency model (namely <strong>linearizability</strong>) and one kind of fault (<strong>network partitions</strong>, vi or nodes that are alive but disconnected from each other).</p>
</div>
</div>
<div class="sect3">
<h4 id="linearizability-and-network-delays">2.4.2. Linearizability and network delays</h4>
<div class="paragraph">
<p>Although linearizability is a useful guarantee, surprisingly few systems are actually linearizable in practice. For example, even RAM on a modern multi-core CPU is not linearizable: if a thread running on one CPU core writes to a memory address, and a thread on another CPU core reads the same address shortly afterward, it is not guaranteed to read the value written by the first thread (unless a memory barrier or fence is used).</p>
</div>
<div class="paragraph">
<p>The reason for this behavior is that every CPU core has its own memory cache and store buffer. Memory access first goes to the cache by default, and any changes are asynchronously written out to main memory. Since accessing data in the cache is much faster than going to main memory, this feature is essential for good performance on modern CPUs. However, there are now several copies of the data (one in main memory, and perhaps several more in various caches), and these copies are asynchronously updated, so linearizability is lost.</p>
</div>
<div class="paragraph">
<p>Why make this trade-off? It makes no sense to use the CAP theorem to justify the multi-core memory consistency model: within one computer we usually assume reliable communication, and we don’t expect one CPU core to be able to continue operating normally if it is disconnected from the rest of the computer. The reason for dropping linearizability is <strong>performance</strong>, not fault tolerance.</p>
</div>
<div class="paragraph">
<p>The same is true of many distributed databases that choose not to provide linearizable guarantees: they do so primarily to increase performance, not so much for fault tolerance. Linearizability is slow—and this is true all the time, not only during a network fault.</p>
</div>
<div class="paragraph">
<p>Can’t we maybe find a more efficient implementation of linearizable storage? It seems the answer is no: Attiya and Welch prove that if you want linearizability, the response time of read and write requests is at least proportional to the uncertainty of delays in the network. In a network with highly variable delays, like most computer networks, the response time of linearizable reads and writes is inevitably going to be high. <strong>A faster algorithm for linearizability does not exist, but weaker consistency models can be much faster, so this trade-off is important for latency-sensitive systems.</strong></p>
</div>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="ordering-guarantees">3. Ordering Guarantees</h2>
<div class="sectionbody">
<div class="ulist">
<ul>
<li>
<p>The main purpose of the leader in single-leader replication is to determine the <strong>order of writes</strong> in the replication log—that is, the order in which followers apply those writes.</p>
</li>
<li>
<p>Serializability is about ensuring that transactions behave as if they were executed in some <strong>sequential order</strong>. It can be achieved by literally executing transactions in that serial order, or by allowing concurrent execution while preventing serialization conflicts (by locking or aborting).</p>
</li>
<li>
<p>The use of timestamps and clocks in distributed systems is another attempt to introduce order into a disorderly world, for example to determine which one of two writes happened later.</p>
</li>
</ul>
</div>
<div class="sect2">
<h3 id="ordering-and-causality">3.1. Ordering and Causality</h3>
<div class="paragraph">
<p>Causality imposes an ordering on events: cause comes before effect; a message is sent before that message is received; the question comes before the answer. And, like in real life, one thing leads to another: one node reads some data and then writes something as a result, another node reads the thing that was written and writes something else in turn, and so on. These chains of causally dependent operations define the <strong>causal order</strong> in the system—i.e., what happened before what.</p>
</div>
<div class="sect3">
<h4 id="the-causal-order-is-not-a-total-order">3.1.1. The causal order is not a total order</h4>
<div class="paragraph">
<p>A <strong>total order</strong> allows any two elements to be compared, so if you have two elements, you can always say which one is greater and which one is smaller. For example, natural numbers are totally ordered: if I give you any two numbers, say 5 and 13, you can tell me that 13 is greater than 5.</p>
</div>
<div class="paragraph">
<p>However, mathematical sets are not totally ordered: is {a, b} greater than {b, c}? Well, you can’t really compare them, because neither is a subset of the other. We say they are <strong>incomparable</strong>, and therefore mathematical sets are <strong>partially ordered</strong>: in some cases one set is greater than another (if one set contains all the elements of another), but in other cases they are incomparable.</p>
</div>
<div class="ulist">
<ul>
<li>
<p><strong>Linearizability</strong></p>
<div class="paragraph">
<p>In a linearizable system, we have a total order of operations: if the system behaves as if there is only a single copy of the data, and every operation is atomic, this means that for any two operations we can always say which one happened first.</p>
</div>
</li>
<li>
<p><strong>Causality</strong></p>
<div class="paragraph">
<p>We said that two operations are concurrent if neither <strong>happened before</strong> the other. Put another way, two events are ordered if they are causally related (one happened before the other), but they are incomparable if they are concurrent. This means that causality defines a partial order, not a total order: some operations are ordered with respect to each other, but some are incomparable.</p>
</div>
</li>
</ul>
</div>
<div class="paragraph">
<p>Therefore, according to this definition, <strong>there are no concurrent operations in a linearizable datastore</strong>: there must be a single timeline along which all operations are totally ordered. There might be several requests waiting to be handled, but the datastore ensures that every request is handled atomically at a single point in time, acting on a single copy of the data, along a single timeline, without any concurrency.</p>
</div>
</div>
<div class="sect3">
<h4 id="linearizability-is-stronger-than-causal-consistency">3.1.2. Linearizability is stronger than causal consistency</h4>
<div class="paragraph">
<p>So what is the relationship between the causal order and linearizability? The answer is any system that is linearizable will preserve causality correctly.</p>
</div>
<div class="paragraph">
<p>The fact that linearizability ensures causality is what makes linearizable systems simple to understand and appealing. However making a system linearizable can harm its performance and availability, especially if the system has significant network delays (for example, if it’s geographically distributed). For this reason, some distributed data systems have abandoned linearizability, which allows them to achieve better performance but can make them difficult to work with.</p>
</div>
<div class="paragraph">
<p>In many cases, systems that appear to require linearizability in fact only really require causal consistency, which can be implemented more efficiently. In fact, <strong>causal consistency</strong> is the strongest possible consistency model that does not slow down due to network delays, and remains available in the face of network failures.</p>
</div>
</div>
<div class="sect3">
<h4 id="capturing-causal-dependencies">3.1.3. Capturing causal dependencies</h4>
<div class="paragraph">
<p>In order to maintain causality, you need to know which operation <strong>happened before</strong> which other operation. This is a <strong>partial order</strong>: concurrent operations may be processed in any order, but if one operation happened before another, then they must be processed in that order on every replica. Thus, when a replica processes an operation, it must ensure that all causally preceding operations (all operations that happened before) have already been processed; if some preceding operation is missing, the later operation must wait until the preceding operation has been processed.</p>
</div>
</div>
</div>
<div class="sect2">
<h3 id="sequence-number-ordering">3.2. Sequence Number Ordering</h3>
<div class="paragraph">
<p>Although causality is an important theoretical concept, actually keeping track of all causal dependencies can become impractical. In many applications, clients read lots of data before writing something, and then it is not clear whether the write is causally dependent on all or only some of those prior reads. Explicitly tracking all the data that has been read would mean a large overhead.</p>
</div>
<div class="paragraph">
<p>However, there is a better way: we can <strong>use sequence numbers or timestamps to order events</strong>. A timestamp need not come from a time-of-day clock (or physical clock, which have many problems. It can instead come from a <strong>logical clock</strong>, which is an algorithm to generate a sequence of numbers to identify operations, typically using counters that are incremented for every operation.</p>
</div>
<div class="paragraph">
<p>Such sequence numbers or timestamps are compact (only a few bytes in size), and they provide a <strong>total order</strong>: that is, every operation has a unique sequence number, and you can always compare two sequence numbers to determine which is greater (i.e., which operation happened later).</p>
</div>
<div class="paragraph">
<p>In particular, we can create sequence numbers in a total order that is consistent with causality: we promise that if operation A causally happened before B, then A occurs before B in the total order (A has a lower sequence number than B). Concurrent operations may be ordered arbitrarily. Such a total order captures all the causality information, but also imposes more ordering than strictly required by causality.</p>
</div>
<div class="paragraph">
<p>In a database with single-leader replication, the replication log defines a total order of write operations that is consistent with causality. The leader can simply increment a counter for each operation, and thus assign a monotonically increasing sequence number to each operation in the replication log. If a follower applies the writes in the order they appear in the replication log, the state of the follower is always causally consistent (even if it is lagging behind the leader).</p>
</div>
<div class="sect3">
<h4 id="noncausal-sequence-number-generators">3.2.1. Noncausal sequence number generators</h4>
<div class="paragraph">
<p>If there is not a single leader (perhaps because you are using a multi-leader or leaderless database, or because the database is partitioned), it is less clear how to generate sequence numbers for operations. Various methods are used in practice:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>Each node can generate its own independent set of sequence numbers. For example, if you have two nodes, one node can generate only odd numbers and the other only even numbers. In general, you could reserve some bits in the binary representation of the sequence number to contain a unique node identifier, and this would ensure that two different nodes can never generate the same sequence number.</p>
</li>
<li>
<p>You can attach a timestamp from a time-of-day clock (physical clock) to each operation. Such timestamps are not sequential, but if they have sufficiently high resolution, they might be sufficient to totally order operations. This fact is used in the last write wins conflict resolution method.</p>
</li>
<li>
<p>You can preallocate blocks of sequence numbers. For example, node A might claim the block of sequence numbers from 1 to 1,000, and node B might claim the block from 1,001 to 2,000. Then each node can independently assign sequence numbers from its block, and allocate a new block when its supply of sequence numbers begins to run low.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>These three options all perform better and are more <strong>scalable</strong> than pushing all operations through a single leader that increments a counter. They generate a unique, approximately increasing sequence number for each operation. However, they all have a problem: the sequence numbers they generate are not consistent with causality.</p>
</div>
</div>
<div class="sect3">
<h4 id="lamport-timestamps">3.2.2. Lamport timestamps</h4>
<div class="paragraph">
<p>Although the three sequence number generators just described are inconsistent with causality, there is actually a simple method for generating sequence numbers that is consistent with causality. It is called a <strong>Lamport timestamp</strong>, proposed in 1978 by Leslie Lamport, in what is now one of the most-cited papers in the field of distributed systems.</p>
</div>
<div class="paragraph">
<p>The use of Lamport timestamps is illustrated in Figure 9-8. Each node has a unique identifier, and each node keeps a counter of the number of operations it has processed. The Lamport timestamp is then simply a pair of <strong>(counter, node ID)</strong>. Two nodes may sometimes have the same counter value, but by including the node ID in the timestamp, each timestamp is made unique.</p>
</div>
<div class="imageblock">
<div class="content">
<img src="/assets/ddia/consistency-and-consensus/Figure_9-8_lamport_timestamps.png" alt="Figure 9 8 lamport timestamps" width="75%" height="75%">
</div>
</div>
<div class="paragraph">
<p>A Lamport timestamp bears no relationship to a physical time-of-day clock, but it provides total ordering: if you have two timestamps, the one with a greater counter value is the greater timestamp; if the counter values are the same, the one with the greater node ID is the greater timestamp.</p>
</div>
<div class="paragraph">
<p>The key idea about Lamport timestamps, which makes them consistent with causality, is the following: every node and every client keeps track of the maximum counter value it has seen so far, and includes that maximum on every request. When a node receives a request or response with a maximum counter value greater than its own counter value, it immediately increases its own counter to that maximum.</p>
</div>
</div>
<div class="sect3">
<h4 id="timestamp-ordering-is-not-sufficient">3.2.3. Timestamp ordering is not sufficient</h4>
<div class="paragraph">
<p>Although Lamport timestamps define a total order of operations that is consistent with causality, they are not quite sufficient to solve many common problems in distributed systems.</p>
</div>
<div class="paragraph">
<p>For example, consider a system that needs to ensure that a username uniquely identifies a user account. If two users concurrently try to create an account with the same username, one of the two should succeed and the other should fail.</p>
</div>
<div class="paragraph">
<p>At first glance, it seems as though a total ordering of operations (e.g., using Lamport timestamps) should be sufficient to solve this problem: if two accounts with the same username are created, pick the one with the lower timestamp as the winner (the one who grabbed the username first), and let the one with the greater timestamp fail. Since timestamps are totally ordered, this comparison is always valid.</p>
</div>
<div class="paragraph">
<p>In order to be sure that no other node is in the process of concurrently creating an account with the same username and a lower timestamp, you would have to check with every other node to see what it is doing. If one of the other nodes has failed or cannot be reached due to a network problem, this system would grind to a halt. This is not the kind of fault-tolerant system that we need.</p>
</div>
<div class="paragraph">
<p>To conclude: in order to implement something like a uniqueness constraint for usernames, it’s not sufficient to have a total ordering of operations—you also need to know when that order is finalized. If you have an operation to create a username, and you are sure that no other node can insert a claim for the same username ahead of your operation in the total order, then you can safely declare the operation successful.</p>
</div>
</div>
</div>
<div class="sect2">
<h3 id="total-order-broadcast">3.3. Total Order Broadcast</h3>
<div class="paragraph">
<p>If your program runs only on a single CPU core, it is easy to define a total ordering of operations: it is simply the order in which they were executed by the CPU. However, in a distributed system, getting all nodes to agree on the same total ordering of operations is tricky.</p>
</div>
<div class="paragraph">
<p><strong>Total order broadcast</strong> or <strong>atomic broadcast</strong> is usually described as a protocol for exchanging messages between nodes. Informally, it requires that two safety properties always be satisfied:</p>
</div>
<div class="ulist">
<ul>
<li>
<p><strong>Reliable delivery</strong></p>
<div class="paragraph">
<p>No messages are lost: if a message is delivered to one node, it is delivered to all nodes.</p>
</div>
</li>
<li>
<p><strong>Totally ordered delivery</strong></p>
<div class="paragraph">
<p>Messages are delivered to every node in the same order.</p>
</div>
</li>
</ul>
</div>
<div class="paragraph">
<p>A correct algorithm for total order broadcast must ensure that the reliability and ordering properties are always satisfied, even if a node or the network is faulty. Of course, messages will not be delivered while the network is interrupted, but an algorithm can keep retrying so that the messages get through when the network is eventually repaired (and then they must still be delivered in the correct order).</p>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
<div class="paragraph">
<p>Partitioned databases with a single leader per partition often maintain ordering only per partition, which means they cannot offer consistency guarantees (e.g., consistent snapshots, foreign key references) across partitions. Total ordering across all partitions is possible, but requires additional coordination.</p>
</div>
</td>
</tr>
</table>
</div>
<div class="sect3">
<h4 id="using-total-order-broadcast">3.3.1. Using total order broadcast</h4>
<div class="paragraph">
<p><strong>Consensus services</strong> such as <strong>ZooKeeper</strong> and <strong>etcd</strong> actually implement total order broadcast. This fact is a hint that there is a strong connection between total order
broadcast and consensus.</p>
</div>
<div class="paragraph">
<p>Total order broadcast is exactly what you need for <strong>database replication</strong>: if every message represents a write to the database, and every replica processes the same writes in the same order, then the replicas will remain consistent with each other (aside from any temporary replication lag). This principle is known as <strong>state machine replication</strong>.</p>
</div>
<div class="paragraph">
<p>Similarly, total order broadcast can be used to implement <strong>serializable transactions</strong>: if every message represents a deterministic transaction to be executed as a stored procedure, and if every node processes those messages in the same order, then the partitions and replicas of the database are kept consistent with each other.</p>
</div>
<div class="paragraph">
<p>An important aspect of total order broadcast is that the order is fixed at the time the messages are delivered: a node is not allowed to retroactively insert a message into an earlier position in the order if subsequent messages have already been delivered. This fact makes total order broadcast stronger than timestamp ordering.</p>
</div>
<div class="paragraph">
<p>Another way of looking at total order broadcast is that it is a way of creating a log (as in a replication log, transaction log, or write-ahead log): delivering a message is like appending to the log. Since all nodes must deliver the same messages in the same order, all nodes can read the log and see the same sequence of messages.</p>
</div>
<div class="paragraph">
<p>Total order broadcast is also useful for implementing a <strong>lock service</strong> that provides <strong>fencing tokens</strong>. Every request to acquire the lock is appended as a message to the log, and all messages are sequentially numbered in the order they appear in the log. The sequence number can then serve as a fencing token, because it is monotonically increasing. In ZooKeeper, this sequence number is called <strong>zxid</strong>.</p>
</div>
</div>
<div class="sect3">
<h4 id="implementing-linearizable-storage-using-total-order-broadcast">3.3.2. Implementing linearizable storage using total order broadcast</h4>
<div class="paragraph">
<p>Total order broadcast is asynchronous: messages are guaranteed to be delivered reliably in a fixed order, but there is no guarantee about when a message will be delivered (so one recipient may lag behind the others). By contrast, linearizability is a recency guarantee: a read is guaranteed to see the latest value written.</p>
</div>
<div class="paragraph">
<p>However, if you have total order broadcast, you can build linearizable storage on top of it. For example, you can ensure that usernames uniquely identify user accounts.</p>
</div>
<div class="paragraph">
<p>Imagine that for every possible username, you can have a linearizable register with an <strong>atomic compare-and-set operation</strong>. Every register initially has the value null (indicating that the username is not taken). When a user wants to create a username, you execute a compare-and-set operation on the register for that username, setting it to the user account ID, under the condition that the previous register value is null. If multiple users try to concurrently grab the same username, only one of the compare-and-set operations will succeed, because the others will see a value other than null (due to linearizability).</p>
</div>
</div>
<div class="sect3">
<h4 id="implementing-total-order-broadcast-using-linearizable-storage">3.3.3. Implementing total order broadcast using linearizable storage</h4>
<div class="paragraph">
<p>The algorithm is simple: for every message you want to send through total order broadcast, you increment-and-get the linearizable integer, and then attach the value you got from the register as a sequence number to the message. You can then send the message to all nodes (resending any lost messages), and the recipients will deliver the messages consecutively by sequence number.</p>
</div>
<div class="paragraph">
<p>How hard could it be to make a linearizable integer with an atomic increment-and-get operation? As usual, if things never failed, it would be easy: you could just keep it in a variable on one node. The problem lies in handling the situation when network connections to that node are interrupted, and restoring the value when that node fails. In general, if you think hard enough about linearizable sequence number generators, you inevitably end up with a consensus algorithm.</p>
</div>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="distributed-transactions-and-consensus">4. Distributed Transactions and Consensus</h2>
<div class="sectionbody">
<div class="paragraph">
<p>Consensus is one of the most important and fundamental problems in distributed computing. On the surface, it seems simple: informally, the goal is simply to get <strong>several nodes to agree on something</strong>. You might think that this shouldn’t be too hard. Unfortunately, many broken systems have been built in the mistaken belief that this problem is easy to solve.</p>
</div>
<div class="paragraph">
<p>There are a number of situations in which it is important for nodes to agree. For example:</p>
</div>
<div class="ulist">
<ul>
<li>
<p><strong>Leader election</strong></p>
<div class="paragraph">
<p>In a database with single-leader replication, all nodes need to agree on which node is the leader. The leadership position might become contested if some nodes can’t communicate with others due to a network fault. In this case, consensus is important to avoid a bad failover, resulting in a split brain situation in which two nodes both believe themselves to be the leader. If there were two leaders, they would both accept writes and their data would diverge, leading to inconsistency and data loss.</p>
</div>
</li>
<li>
<p><strong>Atomic commit</strong></p>
<div class="paragraph">
<p>In a database that supports transactions spanning several nodes or partitions, we have the problem that a transaction may fail on some nodes but succeed on others. If we want to maintain transaction atomicity, we have to get all nodes to agree on the outcome of the transaction: either they all abort/roll back (if anything goes wrong) or they all commit (if nothing goes wrong). This instance of consensus is known as the atomic commit problem.</p>
</div>
</li>
</ul>
</div>
<div class="admonitionblock tip">
<table>
<tr>
<td class="icon">
<i class="fa icon-tip" title="Tip"></i>
</td>
<td class="content">
<div class="title">The Impossibility of Consensus</div>
<div class="paragraph">
<p>You may have heard about the <strong>FLP</strong> result—named after the authors Fischer, Lynch, and Paterson—which proves that there is no algorithm that is always able to reach consensus if there is a risk that a node may crash. In a distributed system, we must assume that nodes may crash, so reliable consensus is impossible. Yet, here we are, discussing algorithms for achieving consensus. What is going on here?</p>
</div>
<div class="paragraph">
<p>The answer is that the FLP result is proved in the asynchronous system model, a very restrictive model that assumes a deterministic algorithm that cannot use any clocks or timeouts. If the algorithm is allowed to use timeouts, or some other way of identifying suspected crashed nodes (even if the suspicion is sometimes wrong), then consensus becomes solvable. Even just allowing the algorithm to use random numbers is sufficient to get around the impossibility result.</p>
</div>
<div class="paragraph">
<p>Thus, although the FLP result about the impossibility of consensus is of great theoretical importance, distributed systems can usually achieve consensus in practice.</p>
</div>
</td>
</tr>
</table>
</div>
<div class="paragraph">
<p>By learning from two-phase commit (2PC) algorithm, which is the most common way of solving atomic commit, we will then work our way toward better consensus algorithms, such as those used in ZooKeeper (<strong>Zab</strong>) and etcd (<strong>Raft</strong>).</p>
</div>
<div class="sect2">
<h3 id="atomic-commit-and-two-phase-commit-2pc">4.1. Atomic Commit and Two-Phase Commit (2PC)</h3>
<div class="paragraph">
<p>Atomicity prevents failed transactions from littering the database with half-finished results and half-updated state.</p>
</div>
<div class="sect3">
<h4 id="from-single-node-to-distributed-atomic-commit">4.1.1. From single-node to distributed atomic commit</h4>
<div class="paragraph">
<p>For transactions that execute at a single database node, atomicity is commonly implemented by the storage engine. When the client asks the database node to commit the transaction, the database makes the transaction’s writes durable (typically in a write-ahead log) and then appends a commit record to the log on disk. If the database crashes in the middle of this process, the transaction is recovered from the log when the node restarts: if the commit record was successfully written to disk before the crash, the transaction is considered committed; if not, any writes from that transaction are rolled back.</p>
</div>
<div class="paragraph">
<p>Thus, on a single node, transaction commitment crucially depends on the order in which data is durably written to disk: first the data, then the commit record. The key deciding moment for whether the transaction commits or aborts is the moment at which the disk finishes writing the commit record: before that moment, it is still possible to abort (due to a crash), but after that moment, the transaction is committed (even if the database crashes). Thus, it is a single device (the controller of one particular disk drive, attached to one particular node) that makes the commit atomic.</p>
</div>
<div class="paragraph">
<p>A transaction commit must be irrevocable—you are not allowed to change your mind and retroactively abort a transaction after it has been committed. The reason for this rule is that once data has been committed, it becomes visible to other transactions, and thus other clients may start relying on that data; this principle forms the basis of read committed isolation. It is possible for the effects of a committed transaction to later be undone by another, <strong>compensating transaction</strong>. However, from the database’s point of view this is a separate transaction, and thus any cross-transaction correctness requirements are the application’s problem.</p>
</div>
</div>
<div class="sect3">
<h4 id="introduction-to-two-phase-commit">4.1.2. Introduction to two-phase commit</h4>
<div class="paragraph">
<p>Two-phase commit is an algorithm for achieving atomic transaction commit across multiple nodes—i.e., to ensure that either all nodes commit or all nodes abort. It is a classic algorithm in distributed databases.</p>
</div>
<div class="imageblock">
<div class="content">
<img src="/assets/ddia/consistency-and-consensus/Figure_9-9-two-phase-commit.png" alt="Figure 9 9 two phase commit" width="75%" height="75%">
</div>
</div>
<div class="paragraph">
<p>2PC uses a new component that does not normally appear in single-node transactions: a <strong>coordinator</strong> (also known as <strong>transaction manager</strong>). The coordinator is often implemented as a library within the same application process that is requesting the transaction (e.g., embedded in a Java EE container), but it can also be a separate process or service.</p>
</div>
<div class="paragraph">
<p>A 2PC transaction begins with the application reading and writing data on multiple database nodes, as normal. We call these database nodes <strong>participants</strong> in the transaction. When the application is ready to commit, the coordinator begins phase 1: it sends a <strong>prepare request</strong> to each of the nodes, asking them whether they are able to commit. The coordinator then tracks the responses from the participants:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>If all participants reply “yes,” indicating they are ready to commit, then the coordinator sends out a <strong>commit request</strong> in phase 2, and the commit actually takes place.</p>
</li>
<li>
<p>If any of the participants replies “no,” the coordinator sends an <strong>abort request</strong> to all nodes in phase 2.</p>
</li>
</ul>
</div>
<div class="sect4">
<h5 id="a-system-of-promises">A system of promises</h5>
<div class="paragraph">
<p>To understand why it works, we have to break down the process in a bit more detail:</p>
</div>
<div class="olist arabic">
<ol class="arabic">
<li>
<p>When the application wants to begin a distributed transaction, it requests a <strong>transaction ID</strong> from the coordinator. This transaction ID is globally unique.</p>
</li>
<li>
<p>The application begins a single-node transaction on each of the participants, and attaches the globally unique transaction ID to the single-node transaction. All reads and writes are done in one of these single-node transactions. If anything goes wrong at this stage (for example, a node crashes or a request times out), the coordinator or any of the participants can abort.</p>
</li>
<li>
<p>When the application is ready to commit, the coordinator sends a prepare request to all participants, tagged with the global transaction ID. If any of these requests fails or times out, the coordinator sends an abort request for that transaction ID to all participants.</p>
</li>
<li>
<p>When a participant receives the prepare request, it makes sure that it can definitely commit the transaction under all circumstances. This includes writing all transaction data to disk (a crash, a power failure, or running out of disk space is not an acceptable excuse for refusing to commit later), and checking for any conflicts or constraint violations. By replying “yes” to the coordinator, the node promises to commit the transaction without error if requested. In other words, the participant surrenders the right to abort the transaction, but without actually committing it.</p>
</li>
<li>
<p>When the coordinator has received responses to all prepare requests, it makes a definitive decision on whether to commit or abort the transaction (committing only if all participants voted “yes”). The coordinator must write that decision to its transaction log on disk so that it knows which way it decided in case it subsequently crashes. This is called the <strong>commit point</strong>.</p>
</li>
<li>
<p>Once the coordinator’s decision has been written to disk, the commit or abort request is sent to all participants. If this request fails or times out, the coordinator must <strong>retry forever</strong> until it succeeds. There is no more going back: if the decision was to commit, that decision must be enforced, no matter how many retries it takes. If a participant has crashed in the meantime, the transaction will be committed when it recovers—since the participant voted “yes,” it cannot refuse to commit when it recovers.</p>
</li>
</ol>
</div>
</div>
<div class="sect4">
<h5 id="coordinator-failure">Coordinator failure</h5>
<div class="paragraph">
<p>If the coordinator fails before sending the prepare requests, a participant can safely abort the transaction. But once the participant has received a prepare request and voted “yes,” it can no longer abort unilaterally—it must wait to hear back from the coordinator whether the transaction was committed or aborted. If the coordinator crashes or the network fails at this point, the participant can do nothing but wait. A participant’s transaction in this state is called <strong>in doubt</strong> or <strong>uncertain</strong>.</p>
</div>
<div class="imageblock">
<div class="content">
<img src="/assets/ddia/consistency-and-consensus/Figure_9-10_coordinator_crash.png" alt="Figure 9 10 coordinator crash" width="75%" height="75%">
</div>
</div>
<div class="paragraph">
<p>The only way 2PC can complete is by waiting for the coordinator to recover. This is why the coordinator must write its commit or abort decision to a transaction log on disk before sending commit or abort requests to participants: when the coordinator recovers, it determines the status of all in-doubt transactions by reading its transaction log. Any transactions that don’t have a commit record in the coordinator’s log are aborted. Thus, the commit point of 2PC comes down to a regular single-node atomic commit on the coordinator.</p>
</div>
</div>
</div>
<div class="sect3">
<h4 id="distributed-transactions-in-practice">4.1.3. Distributed Transactions in Practice</h4>
<div class="paragraph">
<p>Distributed transactions, especially those implemented with two-phase commit, have a mixed reputation. On the one hand, they are seen as providing an important safety guarantee that would be hard to achieve otherwise; on the other hand, they are criticized for causing operational problems, killing performance, and promising more than they can deliver.</p>
</div>
<div class="ulist">
<ul>
<li>
<p><strong>Database-internal distributed transactions</strong></p>
<div class="paragraph">
<p>Some distributed databases (i.e., databases that use replication and partitioning in their standard configuration) support internal transactions among the nodes of that database.</p>
</div>
</li>
<li>
<p><strong>Heterogeneous distributed transactions</strong></p>
<div class="paragraph">
<p>In a heterogeneous transaction, the participants are two or more different technologies: for example, two databases from different vendors, or even nondatabase systems such as message brokers. A distributed transaction across these systems must ensure atomic commit, even though the systems may be entirely different under the hood.</p>
</div>
</li>
</ul>
</div>
<div class="paragraph">
<p>Database-internal transactions do not have to be compatible with any other system, so they can use any protocol and apply optimizations specific to that particular technology. For that reason, database-internal distributed transactions can often work quite well. On the other hand, transactions spanning heterogeneous technologies are a lot more challenging.</p>
</div>
<div class="ulist">
<ul>
<li>
<p>Exactly-once message processing</p>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
<div class="paragraph">
<p>The traditional approach to synchronizing writes requires distributed transactions across heterogeneous storage systems, which I think is the wrong solution.</p>
</div>
<div class="paragraph">
<p>Transactions within a single storage or stream processing system are feasible, but when data crosses the boundary between different technologies, I believe that an asynchronous event log with idempotent writes is a much more robust and practical approach.</p>
</div>
<div class="paragraph">
<p>The big advantage of log-based integration is <strong>loose coupling</strong> between the various components.</p>
</div>
</td>
</tr>
</table>
</div>
</li>
<li>
<p>XA transactions</p>
</li>
<li>
<p>Holding locks while in doubt</p>
</li>
<li>
<p>Recovering from coordinator failure</p>
</li>
</ul>
</div>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="fault-tolerant-consensus">5. Fault-Tolerant Consensus</h2>
<div class="sectionbody">
<div class="paragraph">
<p>Informally, consensus means getting several nodes to agree on something. For example, if several people concurrently try to book the last seat on an airplane, or the same seat in a theater, or try to register an account with the same username, then a consensus algorithm could be used to determine which one of these mutually incompatible operations should be the winner.</p>
</div>
<div class="paragraph">
<p>The consensus problem is normally formalized as follows: one or more nodes may <strong>propose</strong> values, and the consensus algorithm <strong>decides</strong> on one of those values. In the seat-booking example, when several customers are concurrently trying to buy the last seat, each node handling a customer request may propose the ID of the customer it is serving, and the decision indicates which one of those customers got the seat.</p>
</div>
<div class="paragraph">
<p>In this formalism, a consensus algorithm must satisfy the following properties:</p>
</div>
<div class="ulist">
<ul>
<li>
<p><strong>Uniform agreement</strong></p>
<div class="paragraph">
<p>No two nodes decide differently.</p>
</div>
</li>
<li>
<p><strong>Integrity</strong></p>
<div class="paragraph">
<p>No node decides twice.</p>
</div>
</li>
<li>
<p><strong>Validity</strong></p>
<div class="paragraph">
<p>If a node decides value v, then v was proposed by some node.</p>
</div>
</li>
<li>
<p><strong>Termination</strong></p>
<div class="paragraph">
<p>Every node that does not crash eventually decides some value.</p>
</div>
</li>
</ul>
</div>
<div class="paragraph">
<p>The uniform agreement and integrity properties define the core idea of consensus: everyone decides on the same outcome, and once you have decided, you cannot change your mind. The validity property exists mostly to rule out trivial solutions: for example, you could have an algorithm that always decides null, no matter what was proposed; this algorithm would satisfy the agreement and integrity properties, but not the validity property.</p>
</div>
<div class="paragraph">
<p>If you don’t care about fault tolerance, then satisfying the first three properties is easy: you can just hardcode one node to be the “dictator,” and let that node make all of the decisions. However, if that one node fails, then the system can no longer make any decisions. This is, in fact, what we saw in the case of two-phase commit: if the coordinator fails, in-doubt participants cannot decide whether to commit or abort.</p>
</div>
<div class="paragraph">
<p>The termination property formalizes the idea of fault tolerance. It essentially says that a consensus algorithm cannot simply sit around and do nothing forever—in other words, it must make progress. Even if some nodes fail, the other nodes must still reach a decision.</p>
</div>
<div class="paragraph">
<p>Of course, if all nodes crash and none of them are running, then it is not possible for any algorithm to decide anything. There is a limit to the number of failures that an algorithm can tolerate: in fact, it can be proved that any consensus algorithm requires at least a majority of nodes to be functioning correctly in order to assure termination.</p>
</div>
<div class="sect2">
<h3 id="consensus-algorithms-and-total-order-broadcast">5.1. Consensus algorithms and total order broadcast</h3>
<div class="paragraph">
<p>The best-known fault-tolerant consensus algorithms are Viewstamped Replication (<strong>VSR</strong>), <strong>Paxos</strong>, <strong>Raft</strong>, and <strong>Zab</strong>. There are quite a few similarities between these algorithms, but they are not the same.</p>
</div>
<div class="paragraph">
<p>It’s sufficient to be aware of some of the high-level ideas that they have in common, unless you’re implementing a consensus system yourself (which is probably not advisable—it’s hard.</p>
</div>
<div class="paragraph">
<p>Most of these algorithms actually don’t directly use the formal model described here (proposing and deciding on a single value, while satisfying the agreement, integrity, validity, and termination properties). Instead, they decide on a <strong>sequence</strong> of values, which makes them <strong>total order broadcast</strong> algorithms.</p>
</div>
<div class="paragraph">
<p>Remember that <strong>total order broadcast requires messages to be delivered exactly once, in the same order, to all nodes</strong>. If you think about it, this is equivalent to performing several rounds of consensus: in each round, nodes propose the message that they want to send next, and then decide on the next message to be delivered in the total order.</p>
</div>
<div class="paragraph">
<p>So, total order broadcast is equivalent to repeated rounds of consensus (each consensus decision corresponding to one message delivery):</p>
</div>
<div class="ulist">
<ul>
<li>
<p>Due to the agreement property of consensus, all nodes decide to deliver the same messages in the same order.</p>
</li>
<li>
<p>Due to the integrity property, messages are not duplicated.</p>
</li>
<li>
<p>Due to the validity property, messages are not corrupted and not fabricated out of thin air.</p>
</li>
<li>
<p>Due to the termination property, messages are not lost.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>Viewstamped Replication, Raft, and Zab implement total order broadcast directly, because that is more efficient than doing repeated rounds of one-value-at-a-time consensus. In the case of Paxos, this optimization is known as Multi-Paxos.</p>
</div>
<div class="sect3">
<h4 id="limitations-of-consensus">5.1.1. Limitations of consensus</h4>
<div class="paragraph">
<p>Consensus algorithms are a huge breakthrough for distributed systems: they bring concrete safety properties (agreement, integrity, and validity) to systems where everything else is uncertain, and they nevertheless remain fault-tolerant (able to make progress as long as a majority of nodes are working and reachable). They provide total order broadcast, and therefore they can also implement linearizable atomic operations in a fault-tolerant way.</p>
</div>
<div class="paragraph">
<p>Nevertheless, they are not used everywhere, because the benefits come at a cost.</p>
</div>
<div class="paragraph">
<p>The process by which nodes vote on proposals before they are decided is a kind of synchronous replication.</p>
</div>
<div class="paragraph">
<p>Consensus systems always require a strict majority to operate. This means you need a minimum of three nodes in order to tolerate one failure (the remaining two out of three form a majority), or a minimum of five nodes to tolerate two failures (the remaining three out of five form a majority). If a network failure cuts off some nodes from the rest, only the majority portion of the network can make progress, and the rest is blocked.</p>
</div>
<div class="paragraph">
<p>Most consensus algorithms assume a fixed set of nodes that participate in voting, which means that you can’t just add or remove nodes in the cluster. Dynamic membership extensions to consensus algorithms allow the set of nodes in the cluster to change over time, but they are much less well understood than static membership
algorithms.</p>
</div>
<div class="paragraph">
<p>Consensus systems generally rely on timeouts to detect failed nodes. In environments with highly variable network delays, especially geographically distributed systems, it often happens that a node falsely believes the leader to have failed due to a transient network issue. Although this error does not harm the safety properties, frequent leader elections result in terrible performance because the system can end up spending more time choosing a leader than doing any useful work.</p>
</div>
<div class="paragraph">
<p>Sometimes, consensus algorithms are particularly sensitive to network problems. For example, Raft has been shown to have unpleasant edge cases: if the entire network is working correctly except for one particular network link that is consistently unreliable, Raft can get into situations where leadership continually bounces between two nodes, or the current leader is continually forced to resign, so the system effectively never makes progress. Other consensus algorithms have similar problems, and designing algorithms that are more robust to unreliable networks is still an open research problem.</p>
</div>
</div>
</div>
<div class="sect2">
<h3 id="membership-and-coordination-services">5.2. Membership and Coordination Services</h3>
<div class="paragraph">
<p>Projects like ZooKeeper or etcd are often described as “<strong>distributed key-value stores</strong>” or “<strong>coordination and configuration services</strong>.” The API of such a service looks pretty much like that of a database: you can read and write the value for a given key, and iterate over keys. So if they’re basically databases, why do they go to all the effort of implementing a consensus algorithm? What makes them different from any other kind of database?</p>
</div>
<div class="paragraph">
<p>To understand this, it is helpful to briefly explore how a service like ZooKeeper is used. As an application developer, you will rarely need to use ZooKeeper directly, because it is actually not well suited as a general-purpose database. It is more likely that you will end up relying on it indirectly via some other project: for example, HBase, Hadoop YARN, OpenStack Nova, and Kafka all rely on ZooKeeper running in the background. What is it that these projects get from it?</p>
</div>
<div class="paragraph">
<p><strong>ZooKeeper and etcd are designed to hold small amounts of data that can fit entirely in memory</strong> (although they still write to disk for durability)—so you wouldn’t want to store all of your application’s data here. That small amount of data is replicated across all the nodes using a fault-tolerant total order broadcast algorithm. As discussed previously, total order broadcast is just what you need for database replication: if each message represents a write to the database, applying the same writes in the same order keeps replicas consistent with each other.</p>
</div>
<div class="paragraph">
<p>ZooKeeper is modeled after Google’s Chubby <strong>lock service</strong>, implementing not only total order broadcast (and hence consensus), but also an interesting set of other features that turn out to be particularly useful when building distributed systems:</p>
</div>
<div class="ulist">
<ul>
<li>
<p><strong>Linearizable atomic operations</strong></p>
<div class="paragraph">
<p>Using an atomic compare-and-set operation, you can implement a lock: if several nodes concurrently try to perform the same operation, only one of them will succeed. The consensus protocol guarantees that the operation will be atomic and linearizable, even if a node fails or the network is interrupted at any point. A distributed lock is usually implemented as a <strong>lease</strong>, which has an expiry time so that it is eventually released in case the client fails.</p>
</div>
</li>
<li>
<p><strong>Total ordering of operations</strong></p>
<div class="paragraph">
<p>When some resource is protected by a lock or lease, you need a <strong>fencing token</strong> to prevent clients from conflicting with each other in the case of a process pause. The fencing token is some number that monotonically increases every time the lock is acquired. ZooKeeper provides this by totally ordering all operations and giving each operation a monotonically increasing transaction ID (<strong>zxid</strong>) and version number (<strong>cversion</strong>).</p>
</div>
</li>
<li>
<p><strong>Failure detection</strong></p>
<div class="paragraph">
<p>Clients maintain a long-lived session on ZooKeeper servers, and the client and server periodically exchange heartbeats to check that the other node is still alive. Even if the connection is temporarily interrupted, or a ZooKeeper node fails, the session remains active. However, if the heartbeats cease for a duration that is longer than the session timeout, ZooKeeper declares the session to be dead. Any locks held by a session can be configured to be automatically released when the session times out (ZooKeeper calls these ephemeral nodes).</p>
</div>
</li>
<li>
<p><strong>Change notifications</strong></p>
<div class="paragraph">
<p>Not only can one client read locks and values that were created by another client, but it can also watch them for changes. Thus, a client can find out when another client joins the cluster (based on the value it writes to ZooKeeper), or if another client fails (because its session times out and its ephemeral nodes disappear). By subscribing to notifications, a client avoids having to frequently poll to find out about changes.</p>
</div>
</li>
</ul>
</div>
<div class="paragraph">
<p>Of these features, only the linearizable atomic operations really require consensus. However, it is the combination of these features that makes systems like ZooKeeper so useful for distributed coordination.</p>
</div>
<div class="ulist">
<ul>
<li>
<p>Allocating work to nodes</p>
</li>
<li>
<p>Service discovery</p>
</li>
<li>
<p>Membership services</p>
</li>
</ul>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="references">6. References</h2>
<div class="sectionbody">
<div class="ulist">
<ul>
<li>
<p>Martin Kleppmann: Designing Data-Intensive Applications, O’Reilly, 2017.</p>
</li>
</ul>
</div>
</div>
</div>
  </div>

  <ul class="post-navigation">
    <li>
      
      <a href="/2022/08/08/the-trouble-with-distributed-systems/">&laquo; The Trouble with Distributed Systems</a>
      
    </li>
    <li>
      
      <a href="/2022/08/10/stream-processing/">Stream Processing &raquo;</a>
      
    </li>
  </ul>
</article>

      </div>
    </div>

    <footer class="site-footer">
  <div class="license">
    <span>Article licensed under <a href="http://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a></span>
  </div>
  
  <details open>
    <summary>Extral Links</summary>
    <div>
      
      <a href="https://jekyllrb.com/">Jekyll</a>
      
      &nbsp;.&nbsp;
      
      
      <a href="https://shopify.github.io/">Liquid</a>
      
      &nbsp;.&nbsp;
      
      
      <a href="https://docs.asciidoctor.org/">Asciidoctor</a>
      
      &nbsp;.&nbsp;
      
      
      <a href="https://github.com/qqbuby/">GitHub</a>
      
      &nbsp;.&nbsp;
      
      
      <a href="/feed.xml">RSS</a>
      
      
    </div>
  </details>
  
</footer>


<!-- https://github.com/bryanbraun/anchorjs -->
<script src="/js/anchor.min.js"></script>
<script>
  anchors.add();
  anchors.remove(".site-title");
</script>




  </body>

</html>

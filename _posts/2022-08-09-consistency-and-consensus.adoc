= Consistency and Consensus
:page-layout: post
:page-categories: ['database']
:page-tags: ['database', 'consensus', 'consistency']
:page-date: 2022-08-09 09:48:14 +0800
:page-revdate: 2022-08-09 09:48:14 +0800
:toc:
:sectnums:
:toclevels: 5

The simplest way of handling faults in distributed systems is to simply let the entire service fail, and show the user an error message. If that solution is unacceptable, we need to find ways of *_tolerating_* faults—that is, of keeping the service functioning correctly, even if some internal component is faulty.

The best way of building *_fault-tolerant_* systems is to find some general-purpose abstractions with useful guarantees, implement them once, and then let applications rely on those guarantees. This is the same approach as we used with transactions: by using a transaction, the application can pretend that there are no crashes (*_atomicity_*), that nobody else is concurrently accessing the database (*_isolation_*), and that storage devices are perfectly reliable (*_durability_*). Even though crashes, race conditions, and disk failures do occur, the transaction abstraction hides those problems so that the application doesn’t need to worry about them.

One of the most important abstractions for distributed systems is *_consensus_*: that is, getting all of the nodes to agree on something.

== Consistency Guarantees

If you look at two nodes in a replicated database at the same moment in time, you’re likely to see different data on the two nodes, because write requests arrive on different nodes at different times. These inconsistencies occur no matter what replication method the database uses (single-leader, multi-leader, or leaderless replication).

Most replicated databases provide at least *_eventual consistency_*, which means that if you stop writing to the database and wait for some unspecified length of time, then eventually all read requests will return the same value. In other words, the inconsistency is temporary, and it eventually resolves itself (assuming that any faults in the network are also eventually repaired). A better name for eventual consistency may be *_convergence_*, as we expect all replicas to eventually converge to the same value.

However, this is a very *_weak_* guarantee—it doesn’t say anything about *_when_* the replicas will converge. Until the time of convergence, reads could return anything or nothing.

But, *_stronger consistency models_* don’t come for free: systems with stronger guarantees may have *_worse performance_* or be *_less fault-tolerant_* than systems with weaker guarantees.

== Linearizability

In an eventually consistent database, if you ask two different replicas the same question at the same time, you may get two different answers. That’s confusing.

The idea behind *_linearizability_* (also known as *_atomic consistency_*, *_strong consistency_*, *_immediate consistency_*, or *_external consistency_* is to make a system appear as if there were only one copy of the data, and all operations on it are atomic. With this guarantee, even though there may be multiple replicas in reality, the application does not need to worry about them.

In a linearizable system, as soon as one client successfully completes a write, all clients reading from the database must be able to see the value just written. Maintaining the illusion of a single copy of the data means guaranteeing that the value read is the most recent, up-to-date value, and doesn’t come from a stale cache or replica. In other words, linearizability is a *_recency guarantee_*.

image::/assets/consistency-and-consensus/Figure_9-1_system_not_linearizability.png[,75%,75%]

=== What Makes a System Linearizable?

The basic idea behind linearizability is simple: to make a system appear as if there is *_only a single copy of the data_*.

Figure 9-2 shows three clients concurrently reading and writing the same key _x_ in a linearizable database. In the distributed systems literature, _x_ is called a *_register_*—in practice, it could be one key in a key-value store, one row in a relational database, or one document in a document database, for example.

image::/assets/consistency-and-consensus/Figure_9-2_read_concurrent_with_a_write.png[,75%,75%]

For simplicity, Figure 9-2 shows only the requests from the clients’ point of view, not the internals of the database. Each bar is a request made by a client, where the start of a bar is the time when the request was sent, and the end of a bar is when the response was received by the client. Due to variable network delays, a client doesn’t know exactly when the database processed its request—it only knows that it must have happened sometime between the client sending the request and receiving the response.

In this example, the register has two types of operations:

* `read(x) ⇒ v` means the client requested to read the value of register _x_, and the database returned the value _v_.

* `write(x, v) ⇒ r` means the client requested to set the register _x_ to value _v_, and the database returned response _r_ (which could be _ok_ or _error_).

In Figure 9-2, the value of _x_ is initially 0, and client C performs a write request to set it to 1. While this is happening, clients A and B are repeatedly polling the database to read the latest value. What are the possible responses that A and B might get for their read requests?

* The first read operation by client A completes before the write begins, so it must definitely return the old value 0.

* The last read by client A begins after the write has completed, so it must definitely return the new value 1 if the database is linearizable: we know that the write must have been processed sometime between the start and end of the write operation, and the read must have been processed sometime between the start and end of the read operation. If the read started after the write ended, then the read must have been processed after the write, and therefore it must see the new value that was written.

* Any _read operations that overlap in time with the write operation_ might return either 0 or 1, because we don’t know whether or not the write has taken effect at the time when the read operation is processed. These operations are concurrent with the write.

However, that is not yet sufficient to fully describe linearizability: if reads that are concurrent with a write can return either the old or the new value, then readers could see a value *_flip back and forth_* between the old and the new value several times while a write is going on. That is not what we expect of a system that emulates a “single copy of the data.”

image::/assets/consistency-and-consensus/Figure_9-3_read_after_write_new_value.png[,75%,75%]

In a linearizable system we imagine that there must be some point in time (between the start and end of the write operation) at which the value of _x_ *_atomically flips_* from 0 to 1. Thus, if one client’s read returns the new value 1, all subsequent reads must also return the new value, even if the write operation has not yet completed.

image::/assets/consistency-and-consensus/Figure_9-4_visualizing_points_in_time_linearizability.png[,75%,75%]

In Figure 9-4 we add a third type of operation besides read and write:

* `cas(x, vold, vnew) ⇒ r` means the client requested an atomic compare-and-set operation. If the current value of the register _x_ equals vold, it should be atomically set to _vnew_. If _x ≠ vold_ then the operation should leave the register unchanged and return an error. _r_ is the database’s response (_ok_ or _error_).

The requirement of linearizability is that the lines joining up the operation markers always move forward in time (from left to right), never backward. This requirement ensures the recency guarantee: *_once a new value has been written or read, all subsequent reads see the value that was written, until it is overwritten again._*

.Linearizability Versus Serializability
[TIP]
====
Linearizability is easily confused with serializability, as both words seem to mean something like “can be arranged in a sequential order.” However, they are two quite different guarantees, and it is important to distinguish between them:

* *Serializability*
+
Serializability is an *_isolation property of transactions_*, where every transaction may read and write multiple objects (rows, documents, records). It guarantees that transactions behave the same as if they had executed in some serial order (each transaction running to completion before the next transaction starts). It is okay for that serial order to be different from the order in which transactions were actually run.

* *Linearizability*
+
Linearizability is a *_recency guarantee_* on reads and writes of a register (an individual object). It doesn’t group operations together into transactions, so it does not prevent problems such as write skew, unless you take additional measures such as materializing conflicts.

A database may provide both serializability and linearizability, and this combination is known as strict serializability or strong one-copy serializability (strong-1SR).

Implementations of serializability based on *_two-phase locking or actual serial execution are typically linearizable_*.

However, *_serializable snapshot isolation is not linearizable_*: by design, it makes reads from a consistent snapshot, to avoid lock contention between readers and writers. The whole point of a consistent snapshot is that it does not include writes that are more recent than the snapshot, and thus reads from the snapshot are not linearizable.
====

=== Relying on Linearizability

* *Locking and leader election*
+
A system that uses single-leader replication needs to ensure that there is indeed only one leader, not several (split brain). One way of electing a leader is to use a lock: every node that starts up tries to acquire the lock, and the one that succeeds becomes the leader. No matter how this lock is implemented, it must be linearizable: all nodes must agree which node owns the lock; otherwise it is useless.
+
Coordination services like Apache *_ZooKeeper_* and *_etcd_* are often used to implement *_distributed locks_* and *_leader election_*. They use *_consensus algorithms_* to implement *_linearizable_* operations in a *_fault-tolerant_* way.

* *Constraints and uniqueness guarantees*
+
Uniqueness constraints are common in databases: for example, a username or email address must uniquely identify one user, and in a file storage service there cannot be two files with the same path and filename. If you want to enforce this constraint as the data is written (such that if two people try to concurrently create a user or a file with the same name, one of them will be returned an error), you need linearizability.

* *Cross-channel timing dependencies*
+
For example, say you have a website where users can upload a photo, and a background process resizes the photos to lower resolution for faster download (thumbnails). The architecture and dataflow of this system is illustrated in Figure 9-5.
+
The image resizer needs to be explicitly instructed to perform a resizing job, and this instruction is sent from the web server to the resizer via a message queue. The web server doesn’t place the entire photo on the queue, since most message brokers are designed for small messages, and a photo may be several megabytes in size. Instead, the photo is first written to a file storage service, and once the write is complete, the instruction to the resizer is placed on the queue.
+
image::/assets/consistency-and-consensus/Figure_9-5_web_server_cross_channel_race_condition.png[,75%,75%]
+
If the file storage service is linearizable, then this system should work fine. If it is not linearizable, there is the risk of a race condition: the message queue (steps 3 and 4 in Figure 9-5) might be faster than the internal replication inside the storage service. In this case, when the resizer fetches the image (step 5), it might see an old version of the image, or nothing at all. If it processes an old version of the image, the full-size and resized images in the file storage become permanently inconsistent.
+
This problem arises because there are *_two different communication channels_* between the web server and the resizer: the file storage and the message queue. Without the recency guarantee of linearizability, race conditions between these two channels are possible.

=== Implementing Linearizable Systems

Since linearizability essentially means “behave as though there is *_only a single copy of the data, and all operations on it are atomic_*,” the simplest answer would be to really only use a single copy of the data. However, that approach would not be able to tolerate faults: if the node holding that one copy failed, the data would be lost, or at least inaccessible until the node was brought up again.

The most common approach to making a system fault-tolerant is to use replication.

* *Single-leader replication (potentially linearizable)*
+
In a system with single-leader replication, the leader has the primary copy of the data that is used for writes, and the followers maintain backup copies of the data on other nodes. If you *_make reads from the leader_*, or from *_synchronously updated followers_*, they have the potential to be linearizable. However, not every single-leader database is actually linearizable, either by design (e.g., because it uses *_snapshot isolation_*) or due to *_concurrency bugs_*.
+
Using the leader for reads relies on the assumption that you know for sure who the leader is. It is quite possible for a node to think that it is the leader, when in fact it is not—and if the delusional leader continues to serve requests, it is likely to violate linearizability. With asynchronous replication, failover may even lose committed writes, which violates both durability and linearizability.

* *Consensus algorithms (linearizable)*
+
Some consensus algorithms bear a resemblance to single-leader replication. However, consensus protocols contain measures to prevent split brain and stale replicas. Thanks to these details, consensus algorithms can implement linearizable storage safely. This is how ZooKeeper and etcd work, for example.

* *Multi-leader replication (not linearizable)*
+
Systems with multi-leader replication are generally not linearizable, because they concurrently process writes on multiple nodes and asynchronously replicate
them to other nodes. For this reason, they can produce conflicting writes that require resolution. Such conflicts are an artifact of the lack of a single copy of the data.

* *Leaderless replication (probably not linearizable)*
+
For systems with leaderless replication (Dynamo-style), people sometimes claim that you can obtain “strong consistency” by requiring quorum reads and writes (w + r > n). Depending on the exact configuration of the quorums, and depending on how you define strong consistency, this is not quite true.
+
“Last write wins” conflict resolution methods based on time-of-day clocks (e.g., in Cassandra) are almost certainly nonlinearizable, because clock timestamps cannot be guaranteed to be consistent with actual event ordering due to clock skew. Sloppy quorums also ruin any chance of linearizability. Even with strict quorums, nonlinearizable behavior is possible.
+
Intuitively, it seems as though strict quorum reads and writes should be linearizable in a Dynamo-style model. However, when we have variable network delays, it is possible to have race conditions.
+
image::/assets/consistency-and-consensus/Figure_9-6_nonlinearizable_strict_quorum.png[,75%,75%]
+
In summary, it is safest to assume that a leaderless system with Dynamo-style replication does not provide linearizability.

=== The Cost of Linearizability

image:/assets/consistency-and-consensus/Figure_9-7_network_faults_linearizability_availablity.png[,75%,75%]

==== The CAP theorem

* If your application *_requires linearizability_*, and some replicas are disconnected from the other replicas due to a network problem, then some replicas cannot
process requests while they are disconnected: they must either wait until the network problem is fixed, or return an error (either way, they become *_unavailable_*).

* If your application does *_not require linearizability_*, then it can be written in a way that each replica can process requests independently, even if it is disconnected from other replicas (e.g., multi-leader). In this case, the application can remain *_available_* in the face of a network problem, but its behavior is *_not linearizable_*.

Thus, applications that don’t require linearizability can be more tolerant of network problems. This insight is popularly known as the *_CAP theorem_*, named by Eric Brewer in 2000, although the trade-off has been known to designers of distributed databases since the 1970s.

.The Unhelpful CAP Theorem
[NOTE]
====
CAP is sometimes presented as *Consistency*, *Availability*, *Partition tolerance*: pick 2 out of 3. Unfortunately, putting it this way is misleading because network partitions are a kind of fault, so they aren’t something about which you have a choice: they will happen whether you like it or not.

At times when the network is working correctly, a system can provide both consistency (linearizability) and total availability. When a network fault occurs, you have to choose between either linearizability or total availability. Thus, a better way of phrasing CAP would be either Consistent or Available when Partitioned. A more reliable network needs to make this choice less often, but at some point the choice is inevitable.

In discussions of CAP there are several contradictory definitions of the term availability, and the formalization as a theorem does not match its usual meaning. Many so-called “highly available” (fault-tolerant) systems actually do not meet CAP’s idiosyncratic definition of availability. All in all, there is a lot of misunderstanding and confusion around CAP, and it does not help us understand systems better, so CAP is best avoided.
====

The CAP theorem as formally defined [30] is of very narrow scope: it only considers one consistency model (namely *_linearizability_*) and one kind of fault (*_network partitions_*, vi or nodes that are alive but disconnected from each other).

==== Linearizability and network delays

Although linearizability is a useful guarantee, surprisingly few systems are actually linearizable in practice. For example, even RAM on a modern multi-core CPU is not linearizable: if a thread running on one CPU core writes to a memory address, and a thread on another CPU core reads the same address shortly afterward, it is not guaranteed to read the value written by the first thread (unless a memory barrier or fence is used).

The reason for this behavior is that every CPU core has its own memory cache and store buffer. Memory access first goes to the cache by default, and any changes are asynchronously written out to main memory. Since accessing data in the cache is much faster than going to main memory, this feature is essential for good performance on modern CPUs. However, there are now several copies of the data (one in main memory, and perhaps several more in various caches), and these copies are asynchronously updated, so linearizability is lost.

Why make this trade-off? It makes no sense to use the CAP theorem to justify the multi-core memory consistency model: within one computer we usually assume reliable communication, and we don’t expect one CPU core to be able to continue operating normally if it is disconnected from the rest of the computer. The reason for dropping linearizability is *_performance_*, not fault tolerance.

The same is true of many distributed databases that choose not to provide linearizable guarantees: they do so primarily to increase performance, not so much for fault tolerance. Linearizability is slow—and this is true all the time, not only during a network fault.

Can’t we maybe find a more efficient implementation of linearizable storage? It seems the answer is no: Attiya and Welch prove that if you want linearizability, the response time of read and write requests is at least proportional to the uncertainty of delays in the network. In a network with highly variable delays, like most computer networks, the response time of linearizable reads and writes is inevitably going to be high. *_A faster algorithm for linearizability does not exist, but weaker consistency models can be much faster, so this trade-off is important for latency-sensitive systems._*

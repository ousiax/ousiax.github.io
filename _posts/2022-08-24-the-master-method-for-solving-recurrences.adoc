= The master method for solving recurrences
:page-layout: post
:page-categories: ['math']
:page-tags: ['math']
:page-date: 2022-08-24 19:32:33 +0800
:page-revdate: 2022-08-24 19:32:33 +0800
:toc:
:sectnums:

== Mathematical Induction

*Principle of Mathematical Induction*:

Let _P_ be a property of positive integers such that:

> 1. _Basis_ step: _P(1)_ is true, and
>
> 2. _Inductive_ step: if _P(k)_ is true for all _1 ≤ k ≤ n_ then _P(n + 1)_ is true.

Then _P(n)_ is true for all positive integers.

TIP: The premise _P(n)_ in the inductive step is called *Induction Hypothesis*.

The validity of the Principle of Mathematical Induction is obvious. The basis step states that _P(1)_ is true. Then the inductive step implies that _P(2)_ is also true. By the inductive step again we see that _P(3)_ is true, and so on. Consequently the property is true for all positive integers.

TIP: In the basis step we may replace _1_ with some other integer _m_. Then the conclusion is that the property is true for every integer _n_ greater than or equal to _m_.

Example: Prove that the sum of the _n_ first odd positive integers is _n^2^_, i.e., _1 + 3 + 5 + · · · + (2n − 1) = n^2^_

Proof: _n=1, S(1) = 1^2^ = 1_ (1); _n=k, S(k) = (k)^2^ => n=2k+1, S(k+1) = S(k) + 2k + 1 = k^2^ + 2k + 1 = (k+1)^2^_ (2);

== Recursiveness

A definition such that the object defined occurs in the definition is called a *recursive definition*.

For instance, consider the _Fibonacci sequence_

> 0, 1, 1, 2, 3, 5, 8, 13, . . .

It can be defined as a sequence whose two first terms are _F0 = 0_, _F1 = 1_ and each subsequent term is the sum of the two previous ones:

> _Fn = Fn−1 + Fn−2 (for n ≥ 2)._

Other examples:

_Factorial_:

> 1. _0! = 1_
>
> 2. _n! = n · (n − 1)! (n ≥ 1)_

_Power_:

> 1. _a^0^ = 1_
> 2. _a^n^ = an−1 a (n ≥ 1)_

In all these examples we have:

1. A *basis*, where the function is explicitly evaluated for one or more values of its argument.

2. A *recursive step*, stating how to compute the function from its previous values.

== Divide and Conquer

Many useful algorithms are *recursive* in structure: to solve a given problem, they call themselves recursively one or more times to deal with closely related subproblems.

These algorithms typically follow a *divide-and-conquer* approach: they break the problem into several subproblems that are similar to the original problem but smaller in size, solve the subproblems recursively, and then combine these solutions to create a solution to the original problem.

The divide-and-conquer paradigm involves three steps at each level of the recursion:

* *Divide* the problem into a number of subproblems that are smaller instances of the same problem.

* *Conquer* the subproblems by solving them recursively. If the subproblem sizes are small enough, however, just solve the subproblems in a straightforward manner.

* *Combine* the solutions to the subproblems into the solution for the original problem.
+
[source,text]
----
MERGE-SORT(A, p, r)
1  if p < r
2    q = ⌊(p + r) / 2 ⌋
3    MERGE-SORT(A, p, q)
4    MERGE-SORT(A, q, r)
5    MERGE(A, p, q, r)

MERGE(A, p, q, r)
1  n1 = q - p + 1
2  n2 = r - q
3  let L[1..n1 + 1] and R[1..n2] be new arrays
4  for i = 1 to n1
5      L[i] = A[p + i - 1]
6  for j = 1 to n2
7      R[j] = A[q + j] 
8  L[n1 + 1] =  ∞
9  R[n2 + 1] =  ∞
10 i = 1
11 j = 1
12 for k = p to r
13     if L[i] ≤ R[j] 
14         A[k] = L[i]
15         i = i + 1
16     else A[k] = R[j] 
17         j = j + 1
----

=== Analyzing divide-and-conquer algorithms

When an algorithm contains a recursive call to itself, we can often describe its running time by a *recurrence equation* or *recurrence*, which describes the overall running time on a problem of size _n_ in terms of the running time on smaller inputs. We can then use mathematical tools to solve the recurrence and provide bounds on the performance of the algorithm.

A recurrence for the running time of a divide-and-conquer algorithm falls out from the three steps of the basic paradigm.

* We let _T(n)_ be the running time on a problem of size _n_. If the problem size is small enough, say _n ≤ c_ for some constant _c_, the straightforward solution takes constant time, which we write as Θ(1).

* Suppose that our division of the problem yields _a_ subproblems, each of which is _1/b_ the size of the original. 
+
> For _merge sort_, both _a_ and _b_ are _2_, but we shall see many divide-and-conquer algorithms in which _a ≠ b_.
+
It takes time _T(n/b)_ to solve one subproblem of size _n/b_, and so it takes time _a.T(n/b)_ to solve a of them.

* If we take _D(n) time to divide the problem into subproblems and _C(n)_ time to combine the solutions to the subproblems into the solution to the original problem, we get the recurrence
+
> _T(n)_ =
>
> +++&nbsp;&nbsp;&nbsp;&nbsp;+++_O(1) if n ≤ c_,
>
> +++&nbsp;&nbsp;&nbsp;&nbsp;+++_a.T(n/b) + D(n) + C(n) otherwise_.

=== Proof of the master theorem

The master method provides a “cookbook” method for solving recurrences of the form

> _T(n) = a.T(n/b) + f(n)_

where _a ≥ 1_ and _b > 1_ are constants and _f(n)_ is an asymptotically positive function.

For _merge sort_, we see the _T(n)_ that roughly:

> _T(n) = 2T(n/2) + n_

Replacing _n_ with _n/2_ we have _T(n/2) = 2T(n/4) + n/2_, hence

> _T(n) = 2T(n/2) + n = 2(2T(n/4) + n/2) + n = 4T(n/4) + 2n_

Repeating _k_ times we get:

> _T(n) = 2^k^T(n/2^k^) + k.n_

So for _k = log~2~n_ we have

> _T(n) = nT(1) + nlog~2~n = Θ(n.lgn)_

== References

* Thomas H. Cormen, Charles E. Leiserson, Ronald L. Rivest: Introduction to Algorithms, The MIT Press; 4th edition (April 5, 2022)
* CHAPTER 3 Algorithms, Integers, https://sites.math.northwestern.edu/~mlerma/courses/cs310-05s/notes/dm-algor

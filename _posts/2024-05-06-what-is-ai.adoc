= What is AI?
:page-layout: post
:page-categories: ['ai']
:page-tags: ['ai', 'gpt', 'llm']
:page-date: 2024-05-06 09:03:40 +0800
:page-revdate: 2024-05-06 09:03:40 +0800
:toc:
:toclevels: 4
:sectnums:
:sectnumlevels: 4

== What is AI?

Simply put, AI is software that imitates human behaviors and capabilities. Key workloads include: <<ms-training-ai>>

* *Machine learning* - This is often the foundation for an AI system, and is the way we "teach" a computer model to make predictions and draw conclusions from data.

* *Computer vision* - Capabilities within AI to interpret the world visually through cameras, video, and images.

* *Natural language processing* - Capabilities within AI for a computer to interpret written or spoken language, and respond in kind.

* *Document intelligence* - Capabilities within AI that deal with managing, processing, and using high volumes of data found in forms and documents.

* *Knowledge mining* - Capabilities within AI to extract information from large volumes of often unstructured data to create a searchable knowledge store.

* *Generative AI* - Capabilities within AI that create original content in a variety of formats including natural language, image, code, and more.

=== Machine Learning

Machine Learning is the foundation for most AI solutions.

* Since the 1950's, researchers, often known as _data scientists_, have worked on different approaches to AI.

* Most modern applications of AI have their origins in machine learning, a branch of AI that combines _computer science_ and _mathematics_.

*How machine learning works?*

* The answer is, from *data*.
+
In today's world, we create huge volumes of data as we go about our everyday lives. From the text messages, emails, and social media posts we send to the photographs and videos we take on our phones, we generate massive amounts of information. More data still is created by millions of sensors in our homes, cars, cities, public transport infrastructure, and factories.

* Data scientists can use all of that data to train _machine learning models_ that can make predictions and inferences based on the _relationships_ they find in the data.

*Deep learning, machine learning, and AI*

image::https://learn.microsoft.com/en-us/azure/machine-learning/media/concept-deep-learning-vs-machine-learning/ai-vs-machine-learning-vs-deep-learning.png?view=azureml-api-2["Relationship diagram: AI vs. machine learning vs. deep learning", 35%, 35%]

=== Computer Vision

Computer Vision is an area of AI that deals with visual processing.

* _Image Analysis_: capabilities for analyzing images and video, and extracting descriptions, tags, objects, and text.

* _Face_: capabilities that enable you to build face detection and facial recognition solutions.

* _Optical Character Recognition (OCR)_: capabilities for extracting printed or handwritten text from images, enabling access to a digital version of the scanned text.

=== Natural language processing (NLP)

Natural language processing (NLP) is the area of AI that deals with creating software that understands written and spoken language.

* Analyze and interpret text in documents, email messages, and other sources.

* Interpret spoken language, and synthesize speech responses.

* Automatically translate spoken or written phrases between languages.

* Interpret commands and determine appropriate actions.

=== Document Intelligence

Document Intelligence is the area of AI that deals with managing, processing, and using high volumes of a variety of data found in forms and documents.

Document intelligence enables us to create software that can automate processing for contracts, health documents, financial forms and more.

=== Knowledge Mining

Knowledge mining is the term used to describe solutions that involve extracting information from large volumes of often unstructured data to create a searchable knowledge store.

== What is generative AI?

> Generative artificial intelligence (_generative AI_, GenAI, or GAI) is artificial intelligence capable of generating text, images, videos, or other data using generative models, often in response to https://en.wikipedia.org/wiki/Prompt_(natural_language)[prompts].
>
> Improvements in transformer-based deep neural networks, particularly large language models (LLMs), enabled an AI boom of generative AI systems in the early 2020s. These include chatbots such as ChatGPT, Copilot, Gemini and LLaMA, text-to-image artificial intelligence image generation systems such as Stable Diffusion, Midjourney and DALL-E, and text-to-video AI generators such as Sora.
>
> -- From Wikipedia, the free encyclopedia

Artificial Intelligence (AI) imitates human behavior by using machine learning to interact with the environment and execute tasks without explicit directions on what to output. <<ms-training-generative-ai>>

_Generative AI_ describes a category of capabilities within AI that create original content.

* People typically interact with generative AI that has been built into chat applications. One popular example of such an application is https://openai.com/chatgpt[ChatGPT], a chatbot created by OpenAI, an AI research company that partners closely with Microsoft.

* Generative AI applications take in natural language input, and return appropriate responses in a variety of formats including natural language, image, code, audio, and video.

=== Large language models

Generative AI applications are powered by _large language models_ (LLMs), which are a specialized type of machine learning model that you can use to perform natural language processing (NLP) tasks, including:

* Determining sentiment or otherwise classifying natural language text.
* Summarizing text.
* Comparing multiple text sources for semantic similarity.
* Generating new natural language.

==== Transformer models

Machine learning models for natural language processing have evolved over many years. Today's cutting-edge large language models are based on the _transformer_ architecture, which builds on and extends some techniques that have been proven successful in modeling vocabularies to support NLP tasks - and in particular in generating language.

Transformer models are trained with large volumes of text, enabling them to represent the semantic relationships between words and use those relationships to determine _probable_ sequences of text that make sense.

Transformer models with a large enough vocabulary are capable of generating language responses that are tough to distinguish from human responses.

Transformer model architecture consists of two components, or blocks:

* An _encoder_ block that creates semantic representations of the training vocabulary.

* A _decoder_ block that generates new language sequences.

In practice, the specific implementations of the architecture vary – for example,

* the _Bidirectional Encoder Representations from Transformers_ (BERT) model developed by Google to support their search engine uses only the encoder block, while

* the _Generative Pretrained Transformer_ (GPT) model developed by OpenAI uses only the decoder block.

===== Tokenization

The first step in training a transformer model is to decompose the training text into _tokens_ - in other words, identify each unique text value. With a sufficiently large set of training text, a vocabulary of many thousands of tokens could be compiled. For the sake of simplicity, we can think of each distinct word in the training text as a token (though in reality, tokens can be generated for partial words, or combinations of words and punctuation).

===== Embeddings

To create a vocabulary that encapsulates semantic relationships between the tokens, we define contextual vectors, known as _embeddings_, for them.

* Vectors are multi-valued numeric representations of information, for example `[10, 3, 1]` in which each numeric element represents a particular attribute of the information.

* For language tokens, each element of a token's vector represents some semantic attribute of the token.

* The specific categories for the elements of the vectors in a language model are determined during training based on how commonly words are used together or in similar contexts.

It can be useful to think of the elements in a token embedding vector as _coordinates_ in multidimensional space, so that each token occupies a specific "location."

* The closer tokens are to one another along a particular dimension, the more semantically related they are.

* In other words, related words are grouped closer together.

===== Attention

The _encoder_ and _decoder_ blocks in a transformer model include multiple layers that form the neural network for the model. One of the types of layers that is used in both blocks are _attention layers_.

* _Attention_ is a technique used to examine a sequence of text tokens and try to quantify the strength of the relationships between them.

* In particular, _self-attention_ involves considering how other tokens around one particular token influence that token's meaning.

* In an encoder block, each token is carefully examined in context, and an appropriate encoding is determined for its vector embedding. The vector values are based on the relationship between the token and other tokens with which it frequently appears.

* In a decoder block, attention layers are used to predict the next token in a sequence. For each token generated, the model has an attention layer that takes into account the sequence of tokens up to that point. The model considers which of the tokens are the most influential when considering what the next token should be.

Remember that the attention layer is working with numeric vector representations of the tokens, not the actual text.

* In a decoder, the process starts with a sequence of token embeddings representing the text to be completed.

* During training, the goal is to predict the vector for the final token in the sequence based on the preceding tokens.

* The attention layer assigns a numeric _weight_ to each token in the sequence so far. It uses that value to perform a calculation on the weighted vectors that produces an _attention score_ that can be used to calculate a possible vector for the next token.

In practice, a technique called _multi-head attention_ uses different elements of the embeddings to calculate multiple attention scores.

* A neural network is then used to evaluate all possible tokens to determine the most probable token with which to continue the sequence.

* The process continues iteratively for each token in the sequence, with the output sequence so far being used regressively as the input for the next _iteration_ – essentially building the output one token at a time.

What all of this means, is that a transformer model such as GPT-4 (the model behind ChatGPT and Bing) is designed to take in a text input (called a _prompt_) and generate a syntactically correct output (called a _completion_).

* In effect, the “magic” of the model is that it has the ability to string a coherent sentence together.

* This ability doesn't imply any “knowledge” or “intelligence” on the part of the model; just a large vocabulary and the ability to generate meaningful sequences of words.

* What makes a large language model like GPT-4 so powerful however, is the sheer volume of data with which it has been trained (public and licensed data from the Internet) and the complexity of the network.

* This enables the model to generate completions that are based on the relationships between words in the vocabulary on which the model was trained; often generating output that is indistinguishable from a human response to the same prompt.

=== What is Azure OpenAI?

Azure OpenAI Service is Microsoft's cloud solution for deploying, customizing, and hosting large language models, which is a result of the partnership between Microsoft and OpenAI. The service combines Azure's enterprise-grade capabilities with OpenAI's generative AI model capabilities. <<ms-training-azure-openai>><<ms-training-generative-ai-azopenai>>

Azure OpenAI is available for Azure users and consists of four components:

* Pre-trained generative AI models
* Customization capabilities; the ability to fine-tune AI models with your own data
* Built-in tools to detect and mitigate harmful use cases so users can implement AI responsibly
* Enterprise-grade security with role-based access control (RBAC) and private networks

Azure OpenAI Service provides REST API access to OpenAI's powerful language models which can be easily adapted to specific task including but not limited to content generation, summarization, image understanding, semantic search, and natural language to code translation. Users can access the service through REST APIs, Python SDK, or web-based interface in the Azure OpenAI Studio. <<ms-az-oai-overview>>

==== Models

Azure OpenAI supports many models that can serve different needs. These models include:

* *GPT-4 models* are the latest generation of _generative pretrained_ (GPT) models that can generate natural language and code completions based on natural language prompts.
+
The latest most capable Azure OpenAI models, *GPT-4 Turbo*, is a large _multimodal_ model (accepting text or image inputs and generating text) that can solve difficult problems with greater accuracy than any of OpenAI's previous models. <<ms-az-oai-models>>

* *GPT 3.5 models* can generate natural language and code completions based on natural language prompts.
+
In particular, *GPT-35-turbo models* are optimized for chat-based interactions and work well in most generative AI scenarios.

* *Embeddings models* convert text into numeric vectors, and are useful in language analytics scenarios such as comparing text sources for similarities.

* *DALL-E (/ˈdɑːli/) models* are used to generate images based on natural language prompts.

* *Whisper models* can be used for speech to text. <<ms-az-oai-models>>

* *Text to speech models*, currently in preview, can be used to synthesize text to speech. <<ms-az-oai-models>>

==== Prompts & completions

The completions endpoint is the core component of the API service which provides access to the model's text-in, text-out interface. Users simply need to provide an input prompt containing the English text command, and the model will generate a text completion. <<ms-az-oai-overview>>

Here's an example of a simple prompt and completion:

> Prompt: """ count to 5 in a for loop """
>
> Completion: for i in range(1, 6): print(i)

==== Tokens

* Text tokens <<ms-az-oai-overview>>
+
Azure OpenAI processes text by breaking it down into tokens. Tokens can be words or just chunks of characters. For example, the word “hamburger” gets broken up into the tokens “ham”, “bur” and “ger”, while a short and common word like “pear” is a single token. Many tokens start with a whitespace, for example “ hello” and “ bye”.
+
The total number of tokens processed in a given request depends on the length of your input, output and request parameters. The quantity of tokens being processed will also affect your response latency and throughput for the models.

* Image tokens (GPT-4 Turbo with Vision)
+
The token cost of an input image depends on two main factors: the size of the image and the detail setting (low or high) used for each image. 

==== Prompt engineering

The GPT-3, GPT-3.5 and GPT-4 models from OpenAI are prompt-based. With _prompt-based models_, the user interacts with the model by entering a text prompt, to which the model responds with a text completion. This completion is the model’s continuation of the input text. <<ms-az-oai-overview>>

:ms-az-oai-prompt-engineering: https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/prompt-engineering

While these models are extremely powerful, their behavior is also very sensitive to the prompt, that makes {ms-az-oai-prompt-engineering}[prompt engineering] an important skill to develop.

{ms-az-oai-prompt-engineering}[Prompt engineering] is a technique that is both art and science, which involves designing prompts for generative AI models, that utilizes in-context learning (zero shot and few shot) and, with iteration, improves accuracy and relevancy in responses, optimizing the performance of the model. <<ms-az-oai-customizing-llms>>

TIP: Note that with the https://learn.microsoft.com/en-us/azure/ai-services/openai/how-to/chatgpt[Chat Completion API] few-shot learning examples are typically added to the messages array in the form of example user/assistant interactions after the initial system message. <<ms-az-oai-prompt-engineering>>

Prompt construction can be difficult. In practice, the prompt acts to configure the model weights to complete the desired task, but it's more of an art than a science, often requiring experience and intuition to craft a successful prompt.

==== RAG (Retrieval Augmented Generation)

:ms-az-oai-rag: https://learn.microsoft.com/en-us/azure/ai-studio/concepts/retrieval-augmented-generation

{ms-az-oai-rag}[RAG (Retrieval Augmented Generation)] is a method that integrates external data into a Large Language Model prompt to generate relevant responses. <<ms-az-oai-customizing-llms>>

* It is particularly beneficial when using a large corpus of unstructured text based on different topics.

* It allows for answers to be grounded in the organization’s knowledge base (KB), providing a more tailored and accurate response.

RAG is also advantageous when answering questions based on an organization’s private data or when the public data that the model was trained on might have become outdated, that helps ensure that the responses are always up-to-date and relevant, regardless of the changes in the data landscape.

==== Fine-tuning

:ms-az-oai-fine-tuning-considerations: https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/fine-tuning-considerations

{ms-az-oai-fine-tuning-considerations}[Fine-tuning], specifically supervised fine-tuning in this context, is an iterative process that adapts an existing large language model to a provided training set in order to improve performance, teach the model new skills, or reduce latency. <<ms-az-oai-customizing-llms>>

==== Chat Completions vs. Completions

:chat-completions-api: https://platform.openai.com/docs/guides/text-generation/chat-completions-api
:completions-api: https://platform.openai.com/docs/guides/text-generation/completions-api

The {chat-completions-api}[Chat Completions] format was designed specifically for multi-turn conversations, but can be made similar to the {completions-api}[completions] format for nonchat scenarios by constructing a request using a single user message. For example, one can translate from English to French with the following completions prompt: <<ms-az-oai-chatgpt>><<oai-chat-completions>>

```txt
Translate the following English text to French: "{text}"
```

And an equivalent chat prompt would be:

```txt
[{"role": "user", "content": 'Translate the following English text to French: "{text}"'}]
```

Likewise, the completions API can be used to simulate a chat between a user and an assistant by formatting the input accordingly.

The difference between these APIs is the underlying models that are available in each.

[%header,cols="2,3,3"]
|===

|
|Model families
|API endpoint

|Newer models (2023–)
|gpt-4, gpt-4-turbo-preview, gpt-3.5-turbo
|https://api.openai.com/v1/chat/completions

|Updated LEGACY models (2023)
|gpt-3.5-turbo-instruct, babbage-002, davinci-002
|https://api.openai.com/v1/completions

|===

===== Chat Completions API

Chat models take a list of messages as input and return a model-generated message as output. Although the chat format is designed to make _multi-turn_ conversations easy, it’s just as useful for _single-turn_ tasks without any conversation.

An example Chat Completions API call looks like the following:

```sh
curl https://api.openai.com/v1/chat/completions \
  -H "Content-Type: application/json" \
  -H "Authorization: Bearer $OPENAI_API_KEY" \
  -d '{
    "model": "gpt-3.5-turbo",
    "messages": [
      {
        "role": "system",
        "content": "You are a helpful assistant."
      },
      {
        "role": "user",
        "content": "Who won the world series in 2020?"
      },
      {
        "role": "assistant",
        "content": "The Los Angeles Dodgers won the World Series in 2020."
      },
      {
        "role": "user",
        "content": "Where was it played?"
      }
    ]
  }'
```

An example Chat Completions API response looks as follows:

```json
{
  "choices": [
    {
      "finish_reason": "stop",
      "index": 0,
      "message": {
        "content": "The 2020 World Series was played in Texas at Globe Life Field in Arlington.",
        "role": "assistant"
      },
      "logprobs": null
    }
  ],
  "created": 1677664795,
  "id": "chatcmpl-7QyqpwdfhqwajicIEznoc6Q47XAyW",
  "model": "gpt-3.5-turbo-0613",
  "object": "chat.completion",
  "usage": {
    "completion_tokens": 17,
    "prompt_tokens": 57,
    "total_tokens": 74
  }
}
```

To learn more, you can view the full https://platform.openai.com/docs/api-reference/chat[API reference documentation] for the Chat API.

=== GitHub Copilot

GPT models are able to take natural language or code snippets and translate them into code. The OpenAI GPT models are proficient in over a dozen languages, such as C#, JavaScript, Perl, PHP, and is most capable in Python.

GPT models have been trained on both natural language and billions of lines of code from public repositories. The models are able to generate code from natural language instructions such as code comments, and can suggest ways to complete code functions.

Part of the training data for GPT-3 included programming languages, so it's no surprise that GPT models can answer programming questions if asked. What's unique about the _Codex model family_ is that it's more capable across more languages than GPT models.

OpenAI partnered with GitHub to create _GitHub Copilot_, which they call an AI pair programmer. GitHub Copilot integrates the power of OpenAI Codex into a plugin for developer environments like Visual Studio Code.

[bibliography]
== References

* [[[ms-training-ai,1]]] https://learn.microsoft.com/en-us/training/modules/get-started-ai-fundamentals/
* [[[ms-training-generative-ai,2]]] https://learn.microsoft.com/en-us/training/modules/fundamentals-generative-ai/
* [[[ms-training-azure-openai,3]]] https://learn.microsoft.com/en-us/training/modules/explore-azure-openai/
* [[[ms-training-generative-ai-azopenai,4]]] https://learn.microsoft.com/en-us/training/modules/fundamentals-generative-ai/4-azure-openai
* [[[ms-az-oai-models,5]]] https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/models
* [[[ms-az-oai-overview,6]]] https://learn.microsoft.com/en-us/azure/ai-services/openai/overview
* [[[ms-az-oai-customizing-llms,7]]] https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/customizing-llms
* [[[ms-az-oai-prompt-engineering,8]]] https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/prompt-engineering
* [[[ms-az-oai-chatgpt,9]]] https://learn.microsoft.com/en-us/azure/ai-services/openai/how-to/chatgpt
* [[[oai-chat-completions,10]]] https://platform.openai.com/docs/guides/text-generation/chat-completions-vs-completions
* [[[wiki-gai,11]]] https://en.wikipedia.org/wiki/Generative_artificial_intelligence
* [[[wiki-llms,12]]] https://en.wikipedia.org/wiki/Large_language_model
* [[[wiki-Multimodal_learning,13]]] https://en.wikipedia.org/wiki/Multimodal_learning
* [[[zapier-chatgpt,14]]] https://zapier.com/blog/how-does-chatgpt-work/
* [[[zapier-llm,15]]] https://zapier.com/blog/best-llm/
